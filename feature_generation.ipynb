{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- [x] Load all reviews into a data structure in memory\n",
    "- [x] Apply tokenization to each review (base step)\n",
    "- [ ] Choose and apply three pathways through the feature generation process\n",
    "    - _Remember that you will need to justify your choices with reference to the accuracy that is achieved with each feature set_\n",
    "\n",
    "A simple idea for a datastructure that holds the dataset in memory is:\n",
    "```Python\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"rating\": 7,\n",
    "        \"polarity\": 1,\n",
    "        \"contents:\": \"Lorem Ipsum\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Later, this should be a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing_extensions import Self\n",
    "from functools import cached_property\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "POSITIVE_REVIEWS_DIR = \"./data/pos/\"\n",
    "NEGATIVE_REVIEWS_DIR = \"./data/neg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualDataPoint:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.contents = self._extract_contents()\n",
    "\n",
    "    @cached_property\n",
    "    def basename(self) -> str:\n",
    "        return os.path.basename(self.file_path)\n",
    "\n",
    "    @cached_property\n",
    "    def filename(self) -> str:\n",
    "        return os.path.splitext(self.basename)[0]\n",
    "    \n",
    "    def _extract_contents(self) -> str:\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'contents': self.contents,\n",
    "        }\n",
    "\n",
    "\n",
    "class Review(TextualDataPoint):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "        self.id = self._parse_id()\n",
    "        self.rating = self._parse_rating()\n",
    "        self.polarity = self._determine_polarity()\n",
    "\n",
    "    def _parse_id(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[0])\n",
    "\n",
    "    def _parse_rating(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[1])\n",
    "    \n",
    "    def _determine_polarity(self) -> int:\n",
    "        if self.rating is None:\n",
    "            self.rating = self._parse_rating()\n",
    "\n",
    "        if self.rating <= 4:\n",
    "            return 0\n",
    "        elif self.rating >= 7:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected rating: {self.rating}\")\n",
    "\n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'rating': self.rating,\n",
    "            'polarity': self.polarity,\n",
    "        } | super().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSet:\n",
    "\n",
    "    datapoint_class = TextualDataPoint\n",
    "\n",
    "    def __init__(self, datapoints: list[datapoint_class]):\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        # To keep track of the current iteration position.\n",
    "        self.index = 0\n",
    "\n",
    "    def first(self) -> datapoint_class:\n",
    "        return self.datapoints[0]\n",
    "\n",
    "    def last(self) -> datapoint_class:\n",
    "        return self.datapoints[-1]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __iter__(self) -> Self:\n",
    "        # Reset the index whenever starting a new iteration.\n",
    "        self.index = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self) -> datapoint_class:\n",
    "        # Make sure there are more datapoints to yield.\n",
    "        if self.index < len(self.datapoints):\n",
    "            result = self.datapoints[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            # No more datapoints -> raise StopIteration exception.\n",
    "            raise StopIteration\n",
    "\n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        return [\n",
    "            datapoint.as_dict()\n",
    "            for datapoint in self.datapoints\n",
    "        ]\n",
    "\n",
    "\n",
    "class DataSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dirs: list[str]):\n",
    "        super().__init__(None)\n",
    "        self.dirs = dirs\n",
    "        \n",
    "    def load(self) -> Self:\n",
    "        self.datapoints = [\n",
    "            self.datapoint_class(directory + file)\n",
    "            for directory in self.dirs\n",
    "            for file in os.listdir(directory)\n",
    "        ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().as_lower_representation()\n",
    "\n",
    "    def __iter__(self) -> Self:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().__iter__()\n",
    "    \n",
    "\n",
    "class ReviewDataSet(DataSet):\n",
    "\n",
    "    datapoint_class = Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        # We don't want to modify the original dataset.\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "        # Tokenization is the first preprocessing step of most NLP applications.\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            if isinstance(datapoint.contents, list):\n",
    "                # This datapoint has already been tokenized.\n",
    "                continue\n",
    "\n",
    "            datapoint.contents = nltk.word_tokenize(datapoint.contents)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Generation\n",
    "\n",
    "[Stemming (NTLK)](https://www.nltk.org/howto/stem.html)  \n",
    "[A comparison of Stemming Algorithms & Lemmatization Algorithms](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
    "- `PorterStemmer` is apparently one of the most aggresive `nltk` stemmers\n",
    "    - It appears the choice of stemmer has a significant impact on performance\n",
    "- `SnowballStemmer` appears to be a lighter middle-ground\n",
    "- Lemmatizers are usually \"lighter\" than stemmers, but they cannot handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        super().__init__(dataset.datapoints)\n",
    "\n",
    "    def compare_with(self, other_set: Self):\n",
    "        set1_dp1 = self.first().contents\n",
    "        set2_dp1 = other_set.first().contents\n",
    "        max_length_set1 = len(max(set1_dp1, key=len))\n",
    "\n",
    "        print(\"Comparing the first datapoint in feature sets A and B respectively:\")\n",
    "        for token1, token2 in zip(set1_dp1, set2_dp1):\n",
    "            empty_space = \" \" * (max_length_set1 - len(token1))\n",
    "            print(f\"Set A: {token1} {empty_space}| Set B: {token2}\")\n",
    "\n",
    "    def as_inputs_and_targets(self, target_variable_name: str):\n",
    "        inputs = [datapoint.contents for datapoint in self.datapoints]\n",
    "        targets = [getattr(datapoint, target_variable_name)\n",
    "                   for datapoint in self.datapoints]\n",
    "\n",
    "        return np.array(inputs), np.array(targets)\n",
    "\n",
    "    def split_into_train_dev_test_sets(self, target_variable_name: str, dev_test_size: float, random_state: int = 42):\n",
    "        inputs, targets = self.as_inputs_and_targets(target_variable_name)\n",
    "\n",
    "        # Split the data into train and dev+test sets in a ratio of (1-dev_test_size):(dev_test_size).\n",
    "        initial_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=dev_test_size, random_state=random_state)\n",
    "        train_indexes, test_indexes = next(\n",
    "            initial_splitter.split(inputs, targets))\n",
    "\n",
    "        X_train, y_train = inputs[train_indexes], targets[train_indexes]\n",
    "        X_test_dev, y_test_dev = inputs[test_indexes], targets[test_indexes]\n",
    "\n",
    "        # Split the dev+test set into dev and test sets in a 50:50 ratio.\n",
    "        final_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.5, random_state=random_state)\n",
    "        dev_indexes, test_indexes = next(\n",
    "            final_splitter.split(X_test_dev, y_test_dev))\n",
    "\n",
    "        X_dev, y_dev = X_test_dev[dev_indexes], y_test_dev[dev_indexes]\n",
    "        X_test, y_test = X_test_dev[test_indexes], y_test_dev[test_indexes]\n",
    "\n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetGenerator(Preprocessor):\n",
    "\n",
    "    def create_n_grams(self, n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(ngrams(datapoint.contents, n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def create_everygrams(self, max_n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(everygrams(datapoint.contents, max_len=max_n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def to_lowercase(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token.lower() for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self) -> Self:\n",
    "        distinct_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in distinct_stopwords]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self) -> Self: \n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in string.punctuation]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lemmatize(self) -> Self:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [lmtzr.lemmatize(token) for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stem(self) -> Self:\n",
    "        # Making the assumption that all datapoints are in English.\n",
    "        stmr = SnowballStemmer(\"english\")\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [stmr.stem(token) for token in datapoint.contents]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Normalisation\n",
    "\n",
    "- This needs to be much faster.\n",
    "- Are we actually calcuating word frequencies or just word occurences (are these different?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetNormalizer:\n",
    "\n",
    "    def __init__(self, feature_set: FeatureSet):\n",
    "        # We don't want to modify the original feature set.\n",
    "        self.feature_set = copy.deepcopy(feature_set)\n",
    "\n",
    "        self.normalized = False\n",
    "        self.shared_vocabulary = self._collect_shared_vocabulary()\n",
    "\n",
    "    @cached_property\n",
    "    def num_datapoints(self) -> int:\n",
    "        return len(self.feature_set)\n",
    "\n",
    "    def perform_tf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                datapoint.term_frequencies.get(token, 0)\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def perform_tf_idf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "        self.idfs = self._calculate_idfs()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                (datapoint.term_frequencies.get(token, 0) * self.idfs.get(token))\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def peform_ppmi(self) -> FeatureSet:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _collect_shared_vocabulary(self) -> set:\n",
    "        return set(sorted({\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in datapoint.contents\n",
    "        }))\n",
    "    \n",
    "    def _calculate_idfs(self) -> dict:\n",
    "        # We calculate the document frequencies by creating a unique set of tokens for\n",
    "        # each datapoint (i.e., for each document in the set, counting each token once\n",
    "        # per document regardless of its frequency within the document itself). The Counter\n",
    "        # then aggregates these sets across all datapoints, counting the number of documents\n",
    "        # in which each distinct token appears. This gives us the document frequency for\n",
    "        # each term in the `shared_vocabulary`.\n",
    "        document_frequencies = Counter(\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in set(datapoint.contents)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            token: math.log(self.num_datapoints / (doc_frequency + 1))\n",
    "            for token, doc_frequency in document_frequencies.items()\n",
    "        }\n",
    "    \n",
    "    def _calculate_term_frequencies(self) -> None:\n",
    "        for datapoint in self.feature_set:\n",
    "            num_tokens = len(datapoint.contents)\n",
    "            term_occurences = Counter(datapoint.contents)\n",
    "            \n",
    "            # Normalise the term occurences by dividing them by the length of the datapoint.\n",
    "            datapoint.term_frequencies = {\n",
    "                token: count / num_tokens\n",
    "                for token, count in term_occurences.items()\n",
    "            }\n",
    "\n",
    "    def perform_fast_tf_idf(self) -> FeatureSet:\n",
    "        vocab_to_index = {word: idx for idx, word in enumerate(self.shared_vocabulary)}\n",
    "        num_documents = len(self.feature_set)\n",
    "        num_vocab = len(self.shared_vocabulary)\n",
    "        \n",
    "        # TF matrix where each row corresponds to a document, and each column corresponds to a term.\n",
    "        tf_matrix = np.zeros(shape=(num_documents, num_vocab), dtype=float)\n",
    "        \n",
    "        # Document frequency (DF) counter for counting in how many documents a term appears.\n",
    "        df_counter = defaultdict(int)\n",
    "        \n",
    "        # Populate TF matrix and DF counter\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            # Count term occurrences in the document\n",
    "            term_occurrences = Counter(datapoint.contents)\n",
    "            for term, count in term_occurrences.items():\n",
    "                if term in vocab_to_index:\n",
    "                    index = vocab_to_index[term]\n",
    "                    tf_matrix[doc_idx, index] = count  # Raw count for TF (to be normalized later)\n",
    "                    df_counter[term] += 1\n",
    "        \n",
    "        # Normalize TF matrix row-wise (divide by the number of terms in each document)\n",
    "        doc_lengths = np.array([len(dp.contents) for dp in self.feature_set])\n",
    "        tf_matrix = tf_matrix / doc_lengths[:, None]  # Broadcasting division\n",
    "        \n",
    "        # Convert the DF counter into an array of IDF values\n",
    "        # idf_array = np.log((1 + num_documents) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary]))) + 1\n",
    "        idf_array = np.log((num_documents) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary])))\n",
    "        \n",
    "        # TF-IDF calculation by multiplying the TF matrix by the IDF values\n",
    "        # The transpose on idf_array is necessary for broadcasting to correct dimension\n",
    "        tfidf_matrix = tf_matrix * idf_array\n",
    "        \n",
    "        # Update datapoint contents with tf-idf values\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tfidf_matrix[doc_idx, :].tolist()\n",
    "\n",
    "        return self.feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet([POSITIVE_REVIEWS_DIR, NEGATIVE_REVIEWS_DIR]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_b = FeatureSetGenerator(dataset)\\\n",
    "    .remove_punctuation()\\\n",
    "    .lemmatize()\\\n",
    "    .create_n_grams(1)\n",
    "\n",
    "\n",
    "feature_set_c = FeatureSetGenerator(dataset)\\\n",
    "    .remove_punctuation()\\\n",
    "    .stem()\\\n",
    "    .create_n_grams(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_b.compare_with(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = FeatureSetNormalizer(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_feature_set = normalizer.perform_fast_tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = normalized_feature_set.split_into_train_dev_test_sets(\"polarity\", 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
