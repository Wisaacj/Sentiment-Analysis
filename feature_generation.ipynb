{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- [x] Load all reviews into a data structure in memory\n",
    "- [x] Apply tokenization to each review (base step)\n",
    "- [ ] Choose and apply three pathways through the feature generation process\n",
    "    - _Remember that you will need to justify your choices with reference to the accuracy that is achieved with each feature set_\n",
    "\n",
    "A simple idea for a datastructure that holds the dataset in memory is:\n",
    "```Python\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"rating\": 7,\n",
    "        \"polarity\": 1,\n",
    "        \"contents:\": \"Lorem Ipsum\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Later, this should be a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing_extensions import Self\n",
    "from functools import cached_property\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "POSITIVE_REVIEWS_DIR = \"./data/pos/\"\n",
    "NEGATIVE_REVIEWS_DIR = \"./data/neg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualDataPoint:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.contents = self._extract_contents()\n",
    "\n",
    "    @cached_property\n",
    "    def basename(self) -> str:\n",
    "        return os.path.basename(self.file_path)\n",
    "\n",
    "    @cached_property\n",
    "    def filename(self) -> str:\n",
    "        return os.path.splitext(self.basename)[0]\n",
    "    \n",
    "    def _extract_contents(self) -> str:\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'contents': self.contents,\n",
    "        }\n",
    "\n",
    "\n",
    "class Review(TextualDataPoint):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "        self.id = self._parse_id()\n",
    "        self.rating = self._parse_rating()\n",
    "        self.polarity = self._determine_polarity()\n",
    "\n",
    "    def _parse_id(self) -> int:\n",
    "        return self.filename.split(\"_\")[0]\n",
    "\n",
    "    def _parse_rating(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[1])\n",
    "    \n",
    "    def _determine_polarity(self) -> int:\n",
    "        if self.rating is None:\n",
    "            self.rating = self._parse_rating()\n",
    "\n",
    "        if self.rating <= 4:\n",
    "            return 0\n",
    "        elif self.rating >= 7:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected rating: {self.rating}\")\n",
    "\n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'rating': self.rating,\n",
    "            'polarity': self.polarity,\n",
    "        } | super().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSet:\n",
    "\n",
    "    datapoint_class = TextualDataPoint\n",
    "\n",
    "    def __init__(self, datapoints: list[datapoint_class]):\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        # To keep track of the current iteration position.\n",
    "        self.index = 0\n",
    "\n",
    "    def first(self) -> datapoint_class:\n",
    "        return self.datapoints[0]\n",
    "\n",
    "    def last(self) -> datapoint_class:\n",
    "        return self.datapoints[-1]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __iter__(self) -> Self:\n",
    "        # Reset the index whenever starting a new iteration.\n",
    "        self.index = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self) -> datapoint_class:\n",
    "        # Make sure there are more datapoints to yield.\n",
    "        if self.index < len(self.datapoints):\n",
    "            result = self.datapoints[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            # No more datapoints -> raise StopIteration exception.\n",
    "            raise StopIteration\n",
    "\n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        return [\n",
    "            datapoint.as_dict()\n",
    "            for datapoint in self.datapoints\n",
    "        ]\n",
    "\n",
    "\n",
    "class DataSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dirs: list[str]):\n",
    "        super().__init__(None)\n",
    "        self.dirs = dirs\n",
    "        \n",
    "    def load(self) -> Self:\n",
    "        self.datapoints = [\n",
    "            self.datapoint_class(directory + file)\n",
    "            for directory in self.dirs\n",
    "            for file in os.listdir(directory)\n",
    "        ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().as_lower_representation()\n",
    "\n",
    "    def __iter__(self) -> Self:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().__iter__()\n",
    "    \n",
    "\n",
    "class ReviewDataSet(DataSet):\n",
    "\n",
    "    datapoint_class = Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        # We don't want to modify the original dataset.\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "        # Tokenization is the first preprocessing step of most NLP applications.\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            if isinstance(datapoint.contents, list):\n",
    "                # This datapoint has already been tokenized.\n",
    "                continue\n",
    "\n",
    "            datapoint.contents = nltk.word_tokenize(datapoint.contents)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Generation\n",
    "\n",
    "[Stemming (NTLK)](https://www.nltk.org/howto/stem.html)  \n",
    "[A comparison of Stemming Algorithms & Lemmatization Algorithms](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
    "- `PorterStemmer` is apparently one of the most aggresive `nltk` stemmers\n",
    "    - It appears the choice of stemmer has a significant impact on performance\n",
    "- `SnowballStemmer` appears to be a lighter middle-ground\n",
    "- Lemmatizers are usually \"lighter\" than stemmers, but they cannot handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        super().__init__(dataset.datapoints)\n",
    "\n",
    "    def compare_with(self, other_set: Self):\n",
    "        set1_dp1 = self.first().contents\n",
    "        set2_dp1 = other_set.first().contents\n",
    "        max_length_set1 = len(max(set1_dp1, key=len))\n",
    "\n",
    "        print(\"Comparing the first datapoint in feature sets A and B respectively:\")\n",
    "        for token1, token2 in zip(set1_dp1, set2_dp1):\n",
    "            empty_space = \" \" * (max_length_set1 - len(token1))\n",
    "            print(f\"Set A: {token1} {empty_space}| Set B: {token2}\")\n",
    "\n",
    "    def as_inputs_and_targets(self, target_variable_name: str):\n",
    "        inputs = [datapoint.contents for datapoint in self.datapoints]\n",
    "        targets = [getattr(datapoint, target_variable_name)\n",
    "                   for datapoint in self.datapoints]\n",
    "\n",
    "        return np.array(inputs), np.array(targets)\n",
    "\n",
    "    def split_into_train_dev_test_sets(self, target_variable_name: str, dev_test_size: float, random_state: int = 42):\n",
    "        inputs, targets = self.as_inputs_and_targets(target_variable_name)\n",
    "\n",
    "        # Split the data into train and dev+test sets in a ratio of (1-dev_test_size):(dev_test_size).\n",
    "        initial_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=dev_test_size, random_state=random_state)\n",
    "        train_indexes, test_indexes = next(\n",
    "            initial_splitter.split(inputs, targets))\n",
    "\n",
    "        X_train, y_train = inputs[train_indexes], targets[train_indexes]\n",
    "        X_test_dev, y_test_dev = inputs[test_indexes], targets[test_indexes]\n",
    "\n",
    "        # Split the dev+test set into dev and test sets in a 50:50 ratio.\n",
    "        final_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.5, random_state=random_state)\n",
    "        dev_indexes, test_indexes = next(\n",
    "            final_splitter.split(X_test_dev, y_test_dev))\n",
    "\n",
    "        X_dev, y_dev = X_test_dev[dev_indexes], y_test_dev[dev_indexes]\n",
    "        X_test, y_test = X_test_dev[test_indexes], y_test_dev[test_indexes]\n",
    "\n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetGenerator(Preprocessor):\n",
    "\n",
    "    def create_n_grams(self, n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(ngrams(datapoint.contents, n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def create_everygrams(self, max_n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(everygrams(datapoint.contents, max_len=max_n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def to_lowercase(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token.lower() for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self) -> Self:\n",
    "        distinct_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in distinct_stopwords]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self) -> Self: \n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in string.punctuation]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lemmatize(self) -> Self:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [lmtzr.lemmatize(token) for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stem(self) -> Self:\n",
    "        # Making the assumption that all datapoints are in English.\n",
    "        stmr = SnowballStemmer(\"english\")\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [stmr.stem(token) for token in datapoint.contents]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Normalisation\n",
    "\n",
    "- This needs to be much faster.\n",
    "- Are we actually calcuating word frequencies or just word occurences (are these different?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetNormalizer:\n",
    "\n",
    "    def __init__(self, feature_set: FeatureSet):\n",
    "        # We don't want to modify the original feature set.\n",
    "        self.feature_set = copy.deepcopy(feature_set)\n",
    "\n",
    "        self.normalized = False\n",
    "        self.shared_vocabulary = self._collect_shared_vocabulary()\n",
    "\n",
    "    @cached_property\n",
    "    def num_datapoints(self) -> int:\n",
    "        return len(self.feature_set)\n",
    "\n",
    "    def perform_tf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                datapoint.term_frequencies.get(token, 0)\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def perform_tf_idf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "        self.idfs = self._calculate_idfs()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                (datapoint.term_frequencies.get(token, 0) * self.idfs.get(token))\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def peform_ppmi(self) -> FeatureSet:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _collect_shared_vocabulary(self) -> set:\n",
    "        return set(sorted({\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in datapoint.contents\n",
    "        }))\n",
    "    \n",
    "    def _calculate_idfs(self) -> dict:\n",
    "        # We calculate the document frequencies by creating a unique set of tokens for\n",
    "        # each datapoint (i.e., for each document in the set, counting each token once\n",
    "        # per document regardless of its frequency within the document itself). The Counter\n",
    "        # then aggregates these sets across all datapoints, counting the number of documents\n",
    "        # in which each distinct token appears. This gives us the document frequency for\n",
    "        # each term in the `shared_vocabulary`.\n",
    "        document_frequencies = Counter(\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in set(datapoint.contents)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            token: math.log(self.num_datapoints / (doc_frequency + 1))\n",
    "            for token, doc_frequency in document_frequencies.items()\n",
    "        }\n",
    "    \n",
    "    def _calculate_term_frequencies(self) -> None:\n",
    "        for datapoint in self.feature_set:\n",
    "            num_tokens = len(datapoint.contents)\n",
    "            term_occurences = Counter(datapoint.contents)\n",
    "            datapoint.term_frequencies = {\n",
    "                token: count / num_tokens\n",
    "                for token, count in term_occurences.items()\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet([POSITIVE_REVIEWS_DIR, NEGATIVE_REVIEWS_DIR]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_b = FeatureSetGenerator(dataset).remove_punctuation().lemmatize().create_n_grams(1)\n",
    "feature_set_c = FeatureSetGenerator(dataset).remove_punctuation().stem().create_n_grams(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the first datapoint in feature sets A and B respectively:\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: ('work',) | Set B: ('work',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('property',) | Set B: ('properti',)\n",
      "Set A: ('developer',) | Set B: ('develop',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: ('excels',) | Set B: ('excel',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('being',) | Set B: ('be',)\n",
      "Set A: ('unattractive',) | Set B: ('unattract',)\n",
      "Set A: ('unappreciated',) | Set B: ('unappreci',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('desperate',) | Set B: ('desper',)\n",
      "Set A: ('She',) | Set B: ('she',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('also',) | Set B: ('also',)\n",
      "Set A: ('deaf.',) | Set B: ('deaf.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('Her',) | Set B: ('her',)\n",
      "Set A: ('bos',) | Set B: ('boss',)\n",
      "Set A: ('offer',) | Set B: ('offer',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('hire',) | Set B: ('hire',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('somebody',) | Set B: ('somebodi',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('alleviate',) | Set B: ('allevi',)\n",
      "Set A: ('her',) | Set B: ('her',)\n",
      "Set A: ('heavy',) | Set B: ('heavi',)\n",
      "Set A: ('workload',) | Set B: ('workload',)\n",
      "Set A: ('so',) | Set B: ('so',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: ('us',) | Set B: ('use',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('opportunity',) | Set B: ('opportun',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('secure',) | Set B: ('secur',)\n",
      "Set A: ('herself',) | Set B: ('herself',)\n",
      "Set A: ('some',) | Set B: ('some',)\n",
      "Set A: ('male',) | Set B: ('male',)\n",
      "Set A: ('company',) | Set B: ('compani',)\n",
      "Set A: ('Help',) | Set B: ('help',)\n",
      "Set A: ('arrives',) | Set B: ('arriv',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('form',) | Set B: ('form',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('tattooed',) | Set B: ('tattoo',)\n",
      "Set A: ('hoodlum',) | Set B: ('hoodlum',)\n",
      "Set A: ('fresh',) | Set B: ('fresh',)\n",
      "Set A: ('out',) | Set B: ('out',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('prison',) | Set B: ('prison',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('clearly',) | Set B: ('clear',)\n",
      "Set A: ('unsuited',) | Set B: ('unsuit',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('mannered',) | Set B: ('manner',)\n",
      "Set A: ('routine',) | Set B: ('routin',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('office',) | Set B: ('offic',)\n",
      "Set A: ('environment.',) | Set B: ('environment.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('An',) | Set B: ('an',)\n",
      "Set A: ('implicit',) | Set B: ('implicit',)\n",
      "Set A: ('sexual',) | Set B: ('sexual',)\n",
      "Set A: ('tension',) | Set B: ('tension',)\n",
      "Set A: ('develops',) | Set B: ('develop',)\n",
      "Set A: ('between',) | Set B: ('between',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('two',) | Set B: ('two',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('them',) | Set B: ('them',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('determined',) | Set B: ('determin',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('keep',) | Set B: ('keep',)\n",
      "Set A: ('him',) | Set B: ('him',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('despite',) | Set B: ('despit',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('reluctance',) | Set B: ('reluct',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('embrace',) | Set B: ('embrac',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('working',) | Set B: ('work',)\n",
      "Set A: ('week',) | Set B: ('week',)\n",
      "Set A: ('When',) | Set B: ('when',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('edged',) | Set B: ('edg',)\n",
      "Set A: ('out',) | Set B: ('out',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('important',) | Set B: ('import',)\n",
      "Set A: ('contract',) | Set B: ('contract',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: ('wa',) | Set B: ('was',)\n",
      "Set A: ('negotiating',) | Set B: ('negoti',)\n",
      "Set A: ('by',) | Set B: ('by',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('slimy',) | Set B: ('slimi',)\n",
      "Set A: ('colleague',) | Set B: ('colleagu',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: ('exploit',) | Set B: ('exploit',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('criminality',) | Set B: ('crimin',)\n",
      "Set A: ('by',) | Set B: ('by',)\n",
      "Set A: ('having',) | Set B: ('have',)\n",
      "Set A: ('him',) | Set B: ('him',)\n",
      "Set A: ('steal',) | Set B: ('steal',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('contract',) | Set B: ('contract',)\n",
      "Set A: ('back',) | Set B: ('back',)\n",
      "Set A: ('The',) | Set B: ('the',)\n",
      "Set A: ('colleague',) | Set B: ('colleagu',)\n",
      "Set A: ('quickly',) | Set B: ('quick',)\n",
      "Set A: ('realises',) | Set B: ('realis',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('behind',) | Set B: ('behind',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('robbery',) | Set B: ('robberi',)\n",
      "Set A: ('but',) | Set B: ('but',)\n",
      "Set A: ('when',) | Set B: ('when',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('confronts',) | Set B: ('confront',)\n",
      "Set A: ('her',) | Set B: ('her',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('readiness',) | Set B: ('readi',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('punch',) | Set B: ('punch',)\n",
      "Set A: ('people',) | Set B: ('peopl',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('face',) | Set B: ('face',)\n",
      "Set A: ('come',) | Set B: ('come',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('handy',) | Set B: ('handi',)\n",
      "Set A: ('too',) | Set B: ('too',)\n",
      "Set A: ('but',) | Set B: ('but',)\n",
      "Set A: ('this',) | Set B: ('this',)\n",
      "Set A: ('thuggery',) | Set B: ('thuggeri',)\n",
      "Set A: ('come',) | Set B: ('come',)\n",
      "Set A: ('at',) | Set B: ('at',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('price',) | Set B: ('price',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('given',) | Set B: ('given',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: (\"'going\",) | Set B: ('go',)\n",
      "Set A: ('over',) | Set B: ('over',)\n",
      "Set A: ('by',) | Set B: ('by',)\n",
      "Set A: ('some',) | Set B: ('some',)\n",
      "Set A: ('mob',) | Set B: ('mob',)\n",
      "Set A: ('acquaintance',) | Set B: ('acquaint',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('reminder',) | Set B: ('remind',)\n",
      "Set A: ('about',) | Set B: ('about',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('unpaid',) | Set B: ('unpaid',)\n",
      "Set A: ('debt',) | Set B: ('debt',)\n",
      "Set A: ('He',) | Set B: ('he',)\n",
      "Set A: ('formulates',) | Set B: ('formul',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('plan',) | Set B: ('plan',)\n",
      "Set A: ('which',) | Set B: ('which',)\n",
      "Set A: ('utilises',) | Set B: ('utilis',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('unique',) | Set B: ('uniqu',)\n",
      "Set A: ('lip',) | Set B: ('lip',)\n",
      "Set A: ('reading',) | Set B: ('read',)\n",
      "Set A: ('ability',) | Set B: ('abil',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('rip-off',) | Set B: ('rip-off',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('gang',) | Set B: ('gang',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('violent',) | Set B: ('violent',)\n",
      "Set A: ('bank',) | Set B: ('bank',)\n",
      "Set A: ('robber',) | Set B: ('robber',)\n",
      "Set A: ('It',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('now',) | Set B: ('now',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('turn',) | Set B: ('turn',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('enter',) | Set B: ('enter',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('frightening',) | Set B: ('frighten',)\n",
      "Set A: ('new',) | Set B: ('new',)\n",
      "Set A: ('world.',) | Set B: ('world.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('The',) | Set B: ('the',)\n",
      "Set A: ('fourth',) | Set B: ('fourth',)\n",
      "Set A: ('feature',) | Set B: ('featur',)\n",
      "Set A: ('from',) | Set B: ('from',)\n",
      "Set A: ('director',) | Set B: ('director',)\n",
      "Set A: ('Jacques',) | Set B: ('jacqu',)\n",
      "Set A: ('Audiard',) | Set B: ('audiard',)\n",
      "Set A: (\"'READ\",) | Set B: ('read',)\n",
      "Set A: ('MY',) | Set B: ('my',)\n",
      "Set A: ('LIPS',) | Set B: ('lip',)\n",
      "Set A: ('begin',) | Set B: ('begin',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('thoroughly',) | Set B: ('thorough',)\n",
      "Set A: ('engaging',) | Set B: ('engag',)\n",
      "Set A: ('romantic',) | Set B: ('romant',)\n",
      "Set A: ('drama',) | Set B: ('drama',)\n",
      "Set A: ('between',) | Set B: ('between',)\n",
      "Set A: ('two',) | Set B: ('two',)\n",
      "Set A: ('marginalised',) | Set B: ('marginalis',)\n",
      "Set A: ('loser',) | Set B: ('loser',)\n",
      "Set A: ('only',) | Set B: ('onli',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('shift',) | Set B: ('shift',)\n",
      "Set A: ('gear',) | Set B: ('gear',)\n",
      "Set A: ('halfway',) | Set B: ('halfway',)\n",
      "Set A: ('through',) | Set B: ('through',)\n",
      "Set A: ('into',) | Set B: ('into',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('edgy',) | Set B: ('edgi',)\n",
      "Set A: ('thriller',) | Set B: ('thriller',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('their',) | Set B: ('their',)\n",
      "Set A: ('symbiotic',) | Set B: ('symbiot',)\n",
      "Set A: ('shortcoming',) | Set B: ('shortcom',)\n",
      "Set A: ('turn',) | Set B: ('turn',)\n",
      "Set A: ('them',) | Set B: ('them',)\n",
      "Set A: ('into',) | Set B: ('into',)\n",
      "Set A: ('winner',) | Set B: ('winner',)\n",
      "Set A: ('The',) | Set B: ('the',)\n",
      "Set A: ('lead',) | Set B: ('lead',)\n",
      "Set A: ('are',) | Set B: ('are',)\n",
      "Set A: ('excellent',) | Set B: ('excel',)\n",
      "Set A: ('effortlessly',) | Set B: ('effortless',)\n",
      "Set A: ('convincing',) | Set B: ('convinc',)\n",
      "Set A: ('u',) | Set B: ('us',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('this',) | Set B: ('this',)\n",
      "Set A: ('odd',) | Set B: ('odd',)\n",
      "Set A: ('couple',) | Set B: ('coupl',)\n",
      "Set A: ('could',) | Set B: ('could',)\n",
      "Set A: ('really',) | Set B: ('realli',)\n",
      "Set A: ('connect',) | Set B: ('connect',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('first',) | Set B: ('first',)\n",
      "Set A: ('meeting',) | Set B: ('meet',)\n",
      "Set A: ('with',) | Set B: ('with',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('enjoyable',) | Set B: ('enjoy',)\n",
      "Set A: ('farce',) | Set B: ('farc',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('which',) | Set B: ('which',)\n",
      "Set A: ('she',) | Set B: ('she',)\n",
      "Set A: ('attempt',) | Set B: ('attempt',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('circumnavigate',) | Set B: ('circumnavig',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('surly',) | Set B: ('sur',)\n",
      "Set A: ('reticence',) | Set B: ('retic',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('jailbird',) | Set B: ('jailbird',)\n",
      "Set A: ('manner',) | Set B: ('manner',)\n",
      "Set A: ('only',) | Set B: ('onli',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('discover',) | Set B: ('discov',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('wa',) | Set B: ('was',)\n",
      "Set A: ('until',) | Set B: ('until',)\n",
      "Set A: ('very',) | Set B: ('veri',)\n",
      "Set A: ('recently',) | Set B: ('recent',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('jailbird',) | Set B: ('jailbird',)\n",
      "Set A: ('Emmanuelle',) | Set B: ('emmanuell',)\n",
      "Set A: ('Devos',) | Set B: ('devo',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('play',) | Set B: ('play',)\n",
      "Set A: ('Carla',) | Set B: ('carla',)\n",
      "Set A: ('ha',) | Set B: ('has',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('almost',) | Set B: ('almost',)\n",
      "Set A: ('exclusive',) | Set B: ('exclus',)\n",
      "Set A: ('ability',) | Set B: ('abil',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('go',) | Set B: ('go',)\n",
      "Set A: ('from',) | Set B: ('from',)\n",
      "Set A: ('dowdy',) | Set B: ('dowdi',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('gorgeous',) | Set B: ('gorgeous',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('back',) | Set B: ('back',)\n",
      "Set A: ('again',) | Set B: ('again',)\n",
      "Set A: ('within',) | Set B: ('within',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('frame',) | Set B: ('frame',)\n",
      "Set A: ('Vincent',) | Set B: ('vincent',)\n",
      "Set A: ('Cassel',) | Set B: ('cassel',)\n",
      "Set A: ('play',) | Set B: ('play',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('cornered',) | Set B: ('corner',)\n",
      "Set A: ('dog',) | Set B: ('dog',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('only',) | Set B: ('onli',)\n",
      "Set A: ('really',) | Set B: ('realli',)\n",
      "Set A: ('seems',) | Set B: ('seem',)\n",
      "Set A: ('at',) | Set B: ('at',)\n",
      "Set A: ('home',) | Set B: ('home',)\n",
      "Set A: ('when',) | Set B: ('when',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('receiving',) | Set B: ('receiv',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('beating',) | Set B: ('beat',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('concocting',) | Set B: ('concoct',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('rip-off',) | Set B: ('rip-off',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('likely',) | Set B: ('like',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('get',) | Set B: ('get',)\n",
      "Set A: ('him',) | Set B: ('him',)\n",
      "Set A: ('killed.',) | Set B: ('killed.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('Like',) | Set B: ('like',)\n",
      "Set A: ('many',) | Set B: ('mani',)\n",
      "Set A: ('French',) | Set B: ('french',)\n",
      "Set A: ('film',) | Set B: ('film',)\n",
      "Set A: (\"'READ\",) | Set B: ('read',)\n",
      "Set A: ('MY',) | Set B: ('my',)\n",
      "Set A: ('LIPS',) | Set B: ('lip',)\n",
      "Set A: ('appears',) | Set B: ('appear',)\n",
      "Set A: ('at',) | Set B: ('at',)\n",
      "Set A: ('first',) | Set B: ('first',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('be',) | Set B: ('be',)\n",
      "Set A: ('about',) | Set B: ('about',)\n",
      "Set A: ('nothing',) | Set B: ('noth',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('particular',) | Set B: ('particular',)\n",
      "Set A: ('until',) | Set B: ('until',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('scratch',) | Set B: ('scratch',)\n",
      "Set A: ('beneath',) | Set B: ('beneath',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('surface',) | Set B: ('surfac',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('find',) | Set B: ('find',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('probably',) | Set B: ('probabl',)\n",
      "Set A: ('about',) | Set B: ('about',)\n",
      "Set A: ('everything',) | Set B: ('everyth',)\n",
      "Set A: ('The',) | Set B: ('the',)\n",
      "Set A: ('only',) | Set B: ('onli',)\n",
      "Set A: ('bum',) | Set B: ('bum',)\n",
      "Set A: ('note',) | Set B: ('note',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('subplot',) | Set B: ('subplot',)\n",
      "Set A: ('concerning',) | Set B: ('concern',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('missing',) | Set B: ('miss',)\n",
      "Set A: ('wife',) | Set B: ('wife',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('Paul',) | Set B: ('paul',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('parole',) | Set B: ('parol',)\n",
      "Set A: ('officer',) | Set B: ('offic',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('device',) | Set B: ('devic',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('seems',) | Set B: ('seem',)\n",
      "Set A: ('contrived',) | Set B: ('contriv',)\n",
      "Set A: ('only',) | Set B: ('onli',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('help',) | Set B: ('help',)\n",
      "Set A: ('steer',) | Set B: ('steer',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('main',) | Set B: ('main',)\n",
      "Set A: ('thrust',) | Set B: ('thrust',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('story',) | Set B: ('stori',)\n",
      "Set A: ('into',) | Set B: ('into',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('neat',) | Set B: ('neat',)\n",
      "Set A: ('little',) | Set B: ('littl',)\n",
      "Set A: ('feelgood',) | Set B: ('feelgood',)\n",
      "Set A: ('cul-de-sac.',) | Set B: ('cul-de-sac.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('It',) | Set B: ('it',)\n",
      "Set A: ('wa',) | Set B: ('was',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('French',) | Set B: ('french',)\n",
      "Set A: (\"'New\",) | Set B: ('new',)\n",
      "Set A: ('Wave',) | Set B: ('wave',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('60',) | Set B: ('60',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('first',) | Set B: ('first',)\n",
      "Set A: ('introduced',) | Set B: ('introduc',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('concept',) | Set B: ('concept',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: (\"'genre\",) | Set B: ('genr',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('film',) | Set B: ('film',)\n",
      "Set A: ('making',) | Set B: ('make',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('I',) | Set B: ('i',)\n",
      "Set A: (\"'ve\",) | Set B: ('ve',)\n",
      "Set A: ('always',) | Set B: ('alway',)\n",
      "Set A: ('felt',) | Set B: ('felt',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('any',) | Set B: ('ani',)\n",
      "Set A: ('medium',) | Set B: ('medium',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('somewhat',) | Set B: ('somewhat',)\n",
      "Set A: ('compromised',) | Set B: ('compromis',)\n",
      "Set A: ('when',) | Set B: ('when',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('have',) | Set B: ('have',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('use',) | Set B: ('use',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('system',) | Set B: ('system',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('label',) | Set B: ('label',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('help',) | Set B: ('help',)\n",
      "Set A: ('define',) | Set B: ('defin',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: ('so',) | Set B: ('so',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('always',) | Set B: ('alway',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('pleasure',) | Set B: ('pleasur',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('discover',) | Set B: ('discov',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('film',) | Set B: ('film',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('seems',) | Set B: ('seem',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('transcend',) | Set B: ('transcend',)\n",
      "Set A: ('genre',) | Set B: ('genr',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('better',) | Set B: ('better',)\n",
      "Set A: ('still',) | Set B: ('still',)\n",
      "Set A: ('defy',) | Set B: ('defi',)\n",
      "Set A: ('it',) | Set B: ('it',)\n"
     ]
    }
   ],
   "source": [
    "feature_set_b.compare_with(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = FeatureSetNormalizer(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_feature_set = normalizer.perform_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32205"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_feature_set.first().contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"no_punc_snowball_stem_2_grams_tf_idf.json\"\n",
    "\n",
    "with open(\"./data/feature_sets/\"+filename, \"w\") as file:\n",
    "    json.dump(normalized_feature_set.as_lower_representation(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting of sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = normalized_feature_set.split_into_train_dev_test_sets(\"polarity\", 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
