{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- [x] Load all reviews into a data structure in memory\n",
    "- [x] Apply tokenization to each review (base step)\n",
    "- [ ] Choose and apply three pathways through the feature generation process\n",
    "    - _Remember that you will need to justify your choices with reference to the accuracy that is achieved with each feature set_\n",
    "\n",
    "A simple idea for a datastructure that holds the dataset in memory is:\n",
    "```Python\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"rating\": 7,\n",
    "        \"polarity\": 1,\n",
    "        \"contents:\": \"Lorem Ipsum\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Later, this should be a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing_extensions import Self\n",
    "from functools import cached_property\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "POSITIVE_REVIEWS_DIR = \"./data/pos/\"\n",
    "NEGATIVE_REVIEWS_DIR = \"./data/neg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualDataPoint:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.contents = self._extract_contents()\n",
    "\n",
    "    @cached_property\n",
    "    def basename(self) -> str:\n",
    "        return os.path.basename(self.file_path)\n",
    "\n",
    "    @cached_property\n",
    "    def filename(self) -> str:\n",
    "        return os.path.splitext(self.basename)[0]\n",
    "    \n",
    "    def _extract_contents(self) -> str:\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'contents': self.contents,\n",
    "        }\n",
    "\n",
    "\n",
    "class Review(TextualDataPoint):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "        self.id = self._parse_id()\n",
    "        self.rating = self._parse_rating()\n",
    "        self.polarity = self._determine_polarity()\n",
    "\n",
    "    def _parse_id(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[0])\n",
    "\n",
    "    def _parse_rating(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[1])\n",
    "    \n",
    "    def _determine_polarity(self) -> int:\n",
    "        if self.rating is None:\n",
    "            self.rating = self._parse_rating()\n",
    "\n",
    "        if self.rating <= 4:\n",
    "            return 0\n",
    "        elif self.rating >= 7:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected rating: {self.rating}\")\n",
    "\n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'rating': self.rating,\n",
    "            'polarity': self.polarity,\n",
    "        } | super().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSet:\n",
    "\n",
    "    datapoint_class = TextualDataPoint\n",
    "\n",
    "    def __init__(self, datapoints: list[datapoint_class]):\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        # To keep track of the current iteration position.\n",
    "        self.index = 0\n",
    "\n",
    "    def first(self) -> datapoint_class:\n",
    "        return self.datapoints[0]\n",
    "\n",
    "    def last(self) -> datapoint_class:\n",
    "        return self.datapoints[-1]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __iter__(self) -> Self:\n",
    "        # Reset the index whenever starting a new iteration.\n",
    "        self.index = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self) -> datapoint_class:\n",
    "        # Make sure there are more datapoints to yield.\n",
    "        if self.index < len(self.datapoints):\n",
    "            result = self.datapoints[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            # No more datapoints -> raise StopIteration exception.\n",
    "            raise StopIteration\n",
    "\n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        return [\n",
    "            datapoint.as_dict()\n",
    "            for datapoint in self.datapoints\n",
    "        ]\n",
    "\n",
    "\n",
    "class DataSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dirs: list[str]):\n",
    "        super().__init__(None)\n",
    "        self.dirs = dirs\n",
    "        \n",
    "    def load(self) -> Self:\n",
    "        self.datapoints = [\n",
    "            self.datapoint_class(directory + file)\n",
    "            for directory in self.dirs\n",
    "            for file in os.listdir(directory)\n",
    "        ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().as_lower_representation()\n",
    "\n",
    "    def __iter__(self) -> Self:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().__iter__()\n",
    "    \n",
    "\n",
    "class ReviewDataSet(DataSet):\n",
    "\n",
    "    datapoint_class = Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        # We don't want to modify the original dataset.\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "        # Tokenization is the first preprocessing step of most NLP applications.\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            if isinstance(datapoint.contents, list):\n",
    "                # This datapoint has already been tokenized.\n",
    "                continue\n",
    "\n",
    "            datapoint.contents = nltk.word_tokenize(datapoint.contents)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Generation\n",
    "\n",
    "[Stemming (NTLK)](https://www.nltk.org/howto/stem.html)  \n",
    "[A comparison of Stemming Algorithms & Lemmatization Algorithms](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
    "- `PorterStemmer` is apparently one of the most aggresive `nltk` stemmers\n",
    "    - It appears the choice of stemmer has a significant impact on performance\n",
    "- `SnowballStemmer` appears to be a lighter middle-ground\n",
    "- Lemmatizers are usually \"lighter\" than stemmers, but they cannot handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet(IterableSet):\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        super().__init__(dataset.datapoints)\n",
    "\n",
    "    def compare_with(self, other_set: Self):\n",
    "        set1_dp1 = self.first().contents\n",
    "        set2_dp1 = other_set.first().contents\n",
    "        max_length_set1 = len(max(set1_dp1, key=len))\n",
    "\n",
    "        print(\"Comparing the first datapoint in feature sets A and B respectively:\")\n",
    "        for token1, token2 in zip(set1_dp1, set2_dp1):\n",
    "            empty_space = \" \" * (max_length_set1 - len(token1))\n",
    "            print(f\"Set A: {token1} {empty_space}| Set B: {token2}\")\n",
    "\n",
    "    def as_inputs_and_targets(self, target_variable_name: str):\n",
    "        inputs = [datapoint.contents for datapoint in self.datapoints]\n",
    "        targets = [getattr(datapoint, target_variable_name)\n",
    "                   for datapoint in self.datapoints]\n",
    "\n",
    "        return np.array(inputs), np.array(targets)\n",
    "\n",
    "    def split_into_train_dev_test_sets(self, target_variable_name: str, dev_test_size: float, random_state: int = 42):\n",
    "        inputs, targets = self.as_inputs_and_targets(target_variable_name)\n",
    "\n",
    "        # Split the data into train and dev+test sets in a ratio of (1-dev_test_size):(dev_test_size).\n",
    "        initial_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=dev_test_size, random_state=random_state)\n",
    "        train_indexes, test_indexes = next(\n",
    "            initial_splitter.split(inputs, targets))\n",
    "\n",
    "        X_train, y_train = inputs[train_indexes], targets[train_indexes]\n",
    "        X_test_dev, y_test_dev = inputs[test_indexes], targets[test_indexes]\n",
    "\n",
    "        # Split the dev+test set into dev and test sets in a 50:50 ratio.\n",
    "        final_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.5, random_state=random_state)\n",
    "        dev_indexes, test_indexes = next(\n",
    "            final_splitter.split(X_test_dev, y_test_dev))\n",
    "\n",
    "        X_dev, y_dev = X_test_dev[dev_indexes], y_test_dev[dev_indexes]\n",
    "        X_test, y_test = X_test_dev[test_indexes], y_test_dev[test_indexes]\n",
    "\n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetGenerator(Preprocessor):\n",
    "\n",
    "    def create_n_grams(self, n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(ngrams(datapoint.contents, n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def create_everygrams(self, max_n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(everygrams(datapoint.contents, max_len=max_n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def to_lowercase(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token.lower() for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self) -> Self:\n",
    "        distinct_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in distinct_stopwords]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self) -> Self: \n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in string.punctuation]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lemmatize(self) -> Self:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [lmtzr.lemmatize(token) for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stem(self) -> Self:\n",
    "        # Making the assumption that all datapoints are in English.\n",
    "        stmr = SnowballStemmer(\"english\")\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [stmr.stem(token) for token in datapoint.contents]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Normalisation\n",
    "\n",
    "- This needs to be much faster.\n",
    "- Are we actually calcuating word frequencies or just word occurences (are these different?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetNormalizer:\n",
    "\n",
    "    def __init__(self, feature_set: FeatureSet):\n",
    "        # We don't want to modify the original feature set.\n",
    "        self.feature_set = copy.deepcopy(feature_set)\n",
    "\n",
    "        self.normalized = False\n",
    "        self.shared_vocabulary = self._collect_shared_vocabulary()\n",
    "\n",
    "    @cached_property\n",
    "    def num_datapoints(self) -> int:\n",
    "        return len(self.feature_set)\n",
    "\n",
    "    def perform_tf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                datapoint.term_frequencies.get(token, 0)\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def perform_tf_idf(self) -> FeatureSet:\n",
    "        self._calculate_term_frequencies()\n",
    "        self.idfs = self._calculate_idfs()\n",
    "\n",
    "        for datapoint in self.feature_set:\n",
    "            datapoint.contents = [\n",
    "                (datapoint.term_frequencies.get(token, 0) * self.idfs.get(token))\n",
    "                for token in self.shared_vocabulary\n",
    "            ]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def peform_ppmi(self) -> FeatureSet:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _collect_shared_vocabulary(self) -> set:\n",
    "        return set(sorted({\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in datapoint.contents\n",
    "        }))\n",
    "    \n",
    "    def _calculate_idfs(self) -> dict:\n",
    "        # We calculate the document frequencies by creating a unique set of tokens for\n",
    "        # each datapoint (i.e., for each document in the set, counting each token once\n",
    "        # per document regardless of its frequency within the document itself). The Counter\n",
    "        # then aggregates these sets across all datapoints, counting the number of documents\n",
    "        # in which each distinct token appears. This gives us the document frequency for\n",
    "        # each term in the `shared_vocabulary`.\n",
    "        document_frequencies = Counter(\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in set(datapoint.contents)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            token: math.log(self.num_datapoints / (doc_frequency + 1))\n",
    "            for token, doc_frequency in document_frequencies.items()\n",
    "        }\n",
    "    \n",
    "    def _calculate_term_frequencies(self) -> None:\n",
    "        for datapoint in self.feature_set:\n",
    "            num_tokens = len(datapoint.contents)\n",
    "            term_occurences = Counter(datapoint.contents)\n",
    "            \n",
    "            # Normalise the term occurences by dividing them by the length of the datapoint.\n",
    "            datapoint.term_frequencies = {\n",
    "                token: count / num_tokens\n",
    "                for token, count in term_occurences.items()\n",
    "            }\n",
    "\n",
    "    def perform_fast_tf_idf(self) -> FeatureSet:\n",
    "        vocab_to_index = {word: idx for idx, word in enumerate(self.shared_vocabulary)}\n",
    "        num_documents = len(self.feature_set)\n",
    "        num_vocab = len(self.shared_vocabulary)\n",
    "        \n",
    "        # TF matrix where each row corresponds to a document, and each column corresponds to a term.\n",
    "        tf_matrix = np.zeros(shape=(num_documents, num_vocab), dtype=float)\n",
    "        \n",
    "        # Document frequency (DF) counter for counting in how many documents a term appears.\n",
    "        df_counter = defaultdict(int)\n",
    "        \n",
    "        # Populate TF matrix and DF counter\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            # Count term occurrences in the document\n",
    "            term_occurrences = Counter(datapoint.contents)\n",
    "            for term, count in term_occurrences.items():\n",
    "                if term in vocab_to_index:\n",
    "                    index = vocab_to_index[term]\n",
    "                    tf_matrix[doc_idx, index] = count  # Raw count for TF (to be normalized later)\n",
    "                    df_counter[term] += 1\n",
    "        \n",
    "        # Normalize TF matrix row-wise (divide by the number of terms in each document)\n",
    "        doc_lengths = np.array([len(dp.contents) for dp in self.feature_set])\n",
    "        tf_matrix = tf_matrix / doc_lengths[:, None]  # Broadcasting division\n",
    "        \n",
    "        # Convert the DF counter into an array of IDF values\n",
    "        # idf_array = np.log((1 + num_documents) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary]))) + 1\n",
    "        idf_array = np.log((num_documents) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary])))\n",
    "        \n",
    "        # TF-IDF calculation by multiplying the TF matrix by the IDF values\n",
    "        # The transpose on idf_array is necessary for broadcasting to correct dimension\n",
    "        tfidf_matrix = tf_matrix * idf_array\n",
    "        \n",
    "        # Update datapoint contents with tf-idf values\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tfidf_matrix[doc_idx, :].tolist()\n",
    "\n",
    "        return self.feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet([POSITIVE_REVIEWS_DIR, NEGATIVE_REVIEWS_DIR]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_b = FeatureSetGenerator(dataset)\\\n",
    "    .remove_punctuation()\\\n",
    "    .lemmatize()\\\n",
    "    .create_n_grams(1)\n",
    "\n",
    "\n",
    "feature_set_c = FeatureSetGenerator(dataset)\\\n",
    "    .remove_punctuation()\\\n",
    "    .stem()\\\n",
    "    .create_n_grams(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the first datapoint in feature sets A and B respectively:\n",
      "Set A: ('Homelessness',) | Set B: ('homeless',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('Houselessness',) | Set B: ('houseless',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('George',) | Set B: ('georg',)\n",
      "Set A: ('Carlin',) | Set B: ('carlin',)\n",
      "Set A: ('stated',) | Set B: ('state',)\n",
      "Set A: ('ha',) | Set B: ('has',)\n",
      "Set A: ('been',) | Set B: ('been',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('issue',) | Set B: ('issu',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('year',) | Set B: ('year',)\n",
      "Set A: ('but',) | Set B: ('but',)\n",
      "Set A: ('never',) | Set B: ('never',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('plan',) | Set B: ('plan',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('help',) | Set B: ('help',)\n",
      "Set A: ('those',) | Set B: ('those',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('street',) | Set B: ('street',)\n",
      "Set A: ('that',) | Set B: ('that',)\n",
      "Set A: ('were',) | Set B: ('were',)\n",
      "Set A: ('once',) | Set B: ('onc',)\n",
      "Set A: ('considered',) | Set B: ('consid',)\n",
      "Set A: ('human',) | Set B: ('human',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('did',) | Set B: ('did',)\n",
      "Set A: ('everything',) | Set B: ('everyth',)\n",
      "Set A: ('from',) | Set B: ('from',)\n",
      "Set A: ('going',) | Set B: ('go',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('school',) | Set B: ('school',)\n",
      "Set A: ('work',) | Set B: ('work',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('vote',) | Set B: ('vote',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('matter',) | Set B: ('matter',)\n",
      "Set A: ('Most',) | Set B: ('most',)\n",
      "Set A: ('people',) | Set B: ('peopl',)\n",
      "Set A: ('think',) | Set B: ('think',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('homeless',) | Set B: ('homeless',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('just',) | Set B: ('just',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('lost',) | Set B: ('lost',)\n",
      "Set A: ('cause',) | Set B: ('caus',)\n",
      "Set A: ('while',) | Set B: ('while',)\n",
      "Set A: ('worrying',) | Set B: ('worri',)\n",
      "Set A: ('about',) | Set B: ('about',)\n",
      "Set A: ('thing',) | Set B: ('thing',)\n",
      "Set A: ('such',) | Set B: ('such',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('racism',) | Set B: ('racism',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('war',) | Set B: ('war',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('Iraq',) | Set B: ('iraq',)\n",
      "Set A: ('pressuring',) | Set B: ('pressur',)\n",
      "Set A: ('kid',) | Set B: ('kid',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('succeed',) | Set B: ('succeed',)\n",
      "Set A: ('technology',) | Set B: ('technolog',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('election',) | Set B: ('elect',)\n",
      "Set A: ('inflation',) | Set B: ('inflat',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('worrying',) | Set B: ('worri',)\n",
      "Set A: ('if',) | Set B: ('if',)\n",
      "Set A: ('they',) | Set B: ('they',)\n",
      "Set A: (\"'ll\",) | Set B: ('ll',)\n",
      "Set A: ('be',) | Set B: ('be',)\n",
      "Set A: ('next',) | Set B: ('next',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('end',) | Set B: ('end',)\n",
      "Set A: ('up',) | Set B: ('up',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('streets.',) | Set B: ('streets.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('But',) | Set B: ('but',)\n",
      "Set A: ('what',) | Set B: ('what',)\n",
      "Set A: ('if',) | Set B: ('if',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('were',) | Set B: ('were',)\n",
      "Set A: ('given',) | Set B: ('given',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('bet',) | Set B: ('bet',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('live',) | Set B: ('live',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('street',) | Set B: ('street',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('month',) | Set B: ('month',)\n",
      "Set A: ('without',) | Set B: ('without',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('luxury',) | Set B: ('luxuri',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('once',) | Set B: ('onc',)\n",
      "Set A: ('had',) | Set B: ('had',)\n",
      "Set A: ('from',) | Set B: ('from',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('home',) | Set B: ('home',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('entertainment',) | Set B: ('entertain',)\n",
      "Set A: ('set',) | Set B: ('set',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('bathroom',) | Set B: ('bathroom',)\n",
      "Set A: ('picture',) | Set B: ('pictur',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('wall',) | Set B: ('wall',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('computer',) | Set B: ('comput',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('everything',) | Set B: ('everyth',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('once',) | Set B: ('onc',)\n",
      "Set A: ('treasure',) | Set B: ('treasur',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('see',) | Set B: ('see',)\n",
      "Set A: ('what',) | Set B: ('what',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('like',) | Set B: ('like',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('be',) | Set B: ('be',)\n",
      "Set A: ('homeless',) | Set B: ('homeless',)\n",
      "Set A: ('That',) | Set B: ('that',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('Goddard',) | Set B: ('goddard',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('lesson.',) | Set B: ('lesson.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('Mel',) | Set B: ('mel',)\n",
      "Set A: ('Brooks',) | Set B: ('brook',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('directs',) | Set B: ('direct',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('star',) | Set B: ('star',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('play',) | Set B: ('play',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('rich',) | Set B: ('rich',)\n",
      "Set A: ('man',) | Set B: ('man',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('ha',) | Set B: ('has',)\n",
      "Set A: ('everything',) | Set B: ('everyth',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('world',) | Set B: ('world',)\n",
      "Set A: ('until',) | Set B: ('until',)\n",
      "Set A: ('deciding',) | Set B: ('decid',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('make',) | Set B: ('make',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('bet',) | Set B: ('bet',)\n",
      "Set A: ('with',) | Set B: ('with',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('sissy',) | Set B: ('sissi',)\n",
      "Set A: ('rival',) | Set B: ('rival',)\n",
      "Set A: ('Jeffery',) | Set B: ('jefferi',)\n",
      "Set A: ('Tambor',) | Set B: ('tambor',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('see',) | Set B: ('see',)\n",
      "Set A: ('if',) | Set B: ('if',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('can',) | Set B: ('can',)\n",
      "Set A: ('live',) | Set B: ('live',)\n",
      "Set A: ('in',) | Set B: ('in',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('street',) | Set B: ('street',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('thirty',) | Set B: ('thirti',)\n",
      "Set A: ('day',) | Set B: ('day',)\n",
      "Set A: ('without',) | Set B: ('without',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('luxury',) | Set B: ('luxuri',)\n",
      "Set A: ('if',) | Set B: ('if',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('succeeds',) | Set B: ('succeed',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('can',) | Set B: ('can',)\n",
      "Set A: ('do',) | Set B: ('do',)\n",
      "Set A: ('what',) | Set B: ('what',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('want',) | Set B: ('want',)\n",
      "Set A: ('with',) | Set B: ('with',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('future',) | Set B: ('futur',)\n",
      "Set A: ('project',) | Set B: ('project',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('making',) | Set B: ('make',)\n",
      "Set A: ('more',) | Set B: ('more',)\n",
      "Set A: ('building',) | Set B: ('build',)\n",
      "Set A: ('The',) | Set B: ('the',)\n",
      "Set A: ('bet',) | Set B: ('bet',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: ('thrown',) | Set B: ('thrown',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('street',) | Set B: ('street',)\n",
      "Set A: ('with',) | Set B: ('with',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('bracelet',) | Set B: ('bracelet',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('leg',) | Set B: ('leg',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('monitor',) | Set B: ('monitor',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('every',) | Set B: ('everi',)\n",
      "Set A: ('move',) | Set B: ('move',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('ca',) | Set B: ('ca',)\n",
      "Set A: (\"n't\",) | Set B: (\"n't\",)\n",
      "Set A: ('step',) | Set B: ('step',)\n",
      "Set A: ('off',) | Set B: ('off',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('sidewalk',) | Set B: ('sidewalk',)\n",
      "Set A: ('He',) | Set B: ('he',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('given',) | Set B: ('given',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('nickname',) | Set B: ('nicknam',)\n",
      "Set A: ('Pepto',) | Set B: ('pepto',)\n",
      "Set A: ('by',) | Set B: ('by',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('vagrant',) | Set B: ('vagrant',)\n",
      "Set A: ('after',) | Set B: ('after',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('written',) | Set B: ('written',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('forehead',) | Set B: ('forehead',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('meet',) | Set B: ('meet',)\n",
      "Set A: ('other',) | Set B: ('other',)\n",
      "Set A: ('character',) | Set B: ('charact',)\n",
      "Set A: ('including',) | Set B: ('includ',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('woman',) | Set B: ('woman',)\n",
      "Set A: ('by',) | Set B: ('by',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('name',) | Set B: ('name',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('Molly',) | Set B: ('molli',)\n",
      "Set A: ('Lesley',) | Set B: ('lesley',)\n",
      "Set A: ('Ann',) | Set B: ('ann',)\n",
      "Set A: ('Warren',) | Set B: ('warren',)\n",
      "Set A: ('an',) | Set B: ('an',)\n",
      "Set A: ('ex-dancer',) | Set B: ('ex-danc',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('got',) | Set B: ('got',)\n",
      "Set A: ('divorce',) | Set B: ('divorc',)\n",
      "Set A: ('before',) | Set B: ('befor',)\n",
      "Set A: ('losing',) | Set B: ('lose',)\n",
      "Set A: ('her',) | Set B: ('her',)\n",
      "Set A: ('home',) | Set B: ('home',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('her',) | Set B: ('her',)\n",
      "Set A: ('pal',) | Set B: ('pal',)\n",
      "Set A: ('Sailor',) | Set B: ('sailor',)\n",
      "Set A: ('Howard',) | Set B: ('howard',)\n",
      "Set A: ('Morris',) | Set B: ('morri',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('Fumes',) | Set B: ('fume',)\n",
      "Set A: ('Teddy',) | Set B: ('teddi',)\n",
      "Set A: ('Wilson',) | Set B: ('wilson',)\n",
      "Set A: ('who',) | Set B: ('who',)\n",
      "Set A: ('are',) | Set B: ('are',)\n",
      "Set A: ('already',) | Set B: ('alreadi',)\n",
      "Set A: ('used',) | Set B: ('use',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('street',) | Set B: ('street',)\n",
      "Set A: ('They',) | Set B: ('they',)\n",
      "Set A: (\"'re\",) | Set B: ('re',)\n",
      "Set A: ('survivor',) | Set B: ('survivor',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('is',) | Set B: ('is',)\n",
      "Set A: (\"n't\",) | Set B: (\"n't\",)\n",
      "Set A: ('He',) | Set B: ('he',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('not',) | Set B: ('not',)\n",
      "Set A: ('used',) | Set B: ('use',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('reaching',) | Set B: ('reach',)\n",
      "Set A: ('mutual',) | Set B: ('mutual',)\n",
      "Set A: ('agreement',) | Set B: ('agreement',)\n",
      "Set A: ('like',) | Set B: ('like',)\n",
      "Set A: ('he',) | Set B: ('he',)\n",
      "Set A: ('once',) | Set B: ('onc',)\n",
      "Set A: ('did',) | Set B: ('did',)\n",
      "Set A: ('when',) | Set B: ('when',)\n",
      "Set A: ('being',) | Set B: ('be',)\n",
      "Set A: ('rich',) | Set B: ('rich',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('fight',) | Set B: ('fight',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('flight',) | Set B: ('flight',)\n",
      "Set A: ('kill',) | Set B: ('kill',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('be',) | Set B: ('be',)\n",
      "Set A: ('killed.',) | Set B: ('killed.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('While',) | Set B: ('while',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('love',) | Set B: ('love',)\n",
      "Set A: ('connection',) | Set B: ('connect',)\n",
      "Set A: ('between',) | Set B: ('between',)\n",
      "Set A: ('Molly',) | Set B: ('molli',)\n",
      "Set A: ('and',) | Set B: ('and',)\n",
      "Set A: ('Bolt',) | Set B: ('bolt',)\n",
      "Set A: ('wa',) | Set B: ('was',)\n",
      "Set A: (\"n't\",) | Set B: (\"n't\",)\n",
      "Set A: ('necessary',) | Set B: ('necessari',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('plot',) | Set B: ('plot',)\n",
      "Set A: ('I',) | Set B: ('i',)\n",
      "Set A: ('found',) | Set B: ('found',)\n",
      "Set A: ('``',) | Set B: ('``',)\n",
      "Set A: ('Life',) | Set B: ('life',)\n",
      "Set A: ('Stinks',) | Set B: ('stink',)\n",
      "Set A: (\"''\",) | Set B: (\"''\",)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('be',) | Set B: ('be',)\n",
      "Set A: ('one',) | Set B: ('one',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('Mel',) | Set B: ('mel',)\n",
      "Set A: ('Brooks',) | Set B: ('brook',)\n",
      "Set A: ('observant',) | Set B: ('observ',)\n",
      "Set A: ('film',) | Set B: ('film',)\n",
      "Set A: ('where',) | Set B: ('where',)\n",
      "Set A: ('prior',) | Set B: ('prior',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('being',) | Set B: ('be',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('comedy',) | Set B: ('comedi',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: ('show',) | Set B: ('show',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('tender',) | Set B: ('tender',)\n",
      "Set A: ('side',) | Set B: ('side',)\n",
      "Set A: ('compared',) | Set B: ('compar',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('his',) | Set B: ('his',)\n",
      "Set A: ('slapstick',) | Set B: ('slapstick',)\n",
      "Set A: ('work',) | Set B: ('work',)\n",
      "Set A: ('such',) | Set B: ('such',)\n",
      "Set A: ('a',) | Set B: ('as',)\n",
      "Set A: ('Blazing',) | Set B: ('blaze',)\n",
      "Set A: ('Saddles',) | Set B: ('saddl',)\n",
      "Set A: ('Young',) | Set B: ('young',)\n",
      "Set A: ('Frankenstein',) | Set B: ('frankenstein',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('Spaceballs',) | Set B: ('spacebal',)\n",
      "Set A: ('for',) | Set B: ('for',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('matter',) | Set B: ('matter',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('show',) | Set B: ('show',)\n",
      "Set A: ('what',) | Set B: ('what',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: (\"'s\",) | Set B: (\"'s\",)\n",
      "Set A: ('like',) | Set B: ('like',)\n",
      "Set A: ('having',) | Set B: ('have',)\n",
      "Set A: ('something',) | Set B: ('someth',)\n",
      "Set A: ('valuable',) | Set B: ('valuabl',)\n",
      "Set A: ('before',) | Set B: ('befor',)\n",
      "Set A: ('losing',) | Set B: ('lose',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('next',) | Set B: ('next',)\n",
      "Set A: ('day',) | Set B: ('day',)\n",
      "Set A: ('or',) | Set B: ('or',)\n",
      "Set A: ('on',) | Set B: ('on',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('other',) | Set B: ('other',)\n",
      "Set A: ('hand',) | Set B: ('hand',)\n",
      "Set A: ('making',) | Set B: ('make',)\n",
      "Set A: ('a',) | Set B: ('a',)\n",
      "Set A: ('stupid',) | Set B: ('stupid',)\n",
      "Set A: ('bet',) | Set B: ('bet',)\n",
      "Set A: ('like',) | Set B: ('like',)\n",
      "Set A: ('all',) | Set B: ('all',)\n",
      "Set A: ('rich',) | Set B: ('rich',)\n",
      "Set A: ('people',) | Set B: ('peopl',)\n",
      "Set A: ('do',) | Set B: ('do',)\n",
      "Set A: ('when',) | Set B: ('when',)\n",
      "Set A: ('they',) | Set B: ('they',)\n",
      "Set A: ('do',) | Set B: ('do',)\n",
      "Set A: (\"n't\",) | Set B: (\"n't\",)\n",
      "Set A: ('know',) | Set B: ('know',)\n",
      "Set A: ('what',) | Set B: ('what',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('do',) | Set B: ('do',)\n",
      "Set A: ('with',) | Set B: ('with',)\n",
      "Set A: ('their',) | Set B: ('their',)\n",
      "Set A: ('money',) | Set B: ('money',)\n",
      "Set A: ('Maybe',) | Set B: ('mayb',)\n",
      "Set A: ('they',) | Set B: ('they',)\n",
      "Set A: ('should',) | Set B: ('should',)\n",
      "Set A: ('give',) | Set B: ('give',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('the',) | Set B: ('the',)\n",
      "Set A: ('homeless',) | Set B: ('homeless',)\n",
      "Set A: ('instead',) | Set B: ('instead',)\n",
      "Set A: ('of',) | Set B: ('of',)\n",
      "Set A: ('using',) | Set B: ('use',)\n",
      "Set A: ('it',) | Set B: ('it',)\n",
      "Set A: ('like',) | Set B: ('like',)\n",
      "Set A: ('Monopoly',) | Set B: ('monopoli',)\n",
      "Set A: ('money.',) | Set B: ('money.',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('br',) | Set B: ('br',)\n",
      "Set A: ('Or',) | Set B: ('or',)\n",
      "Set A: ('maybe',) | Set B: ('mayb',)\n",
      "Set A: ('this',) | Set B: ('this',)\n",
      "Set A: ('film',) | Set B: ('film',)\n",
      "Set A: ('will',) | Set B: ('will',)\n",
      "Set A: ('inspire',) | Set B: ('inspir',)\n",
      "Set A: ('you',) | Set B: ('you',)\n",
      "Set A: ('to',) | Set B: ('to',)\n",
      "Set A: ('help',) | Set B: ('help',)\n",
      "Set A: ('others',) | Set B: ('other',)\n"
     ]
    }
   ],
   "source": [
    "feature_set_b.compare_with(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = FeatureSetNormalizer(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_feature_set = normalizer.perform_fast_tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = normalized_feature_set.split_into_train_dev_test_sets(\"polarity\", 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
