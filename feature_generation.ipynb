{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- [x] Load all reviews into a data structure in memory\n",
    "- [x] Apply tokenization to each review (base step)\n",
    "- [x] Choose and apply three pathways through the feature generation process\n",
    "    - _Remember that you will need to justify your choices with reference to the accuracy that is achieved with each feature set_\n",
    "\n",
    "A simple idea for a datastructure that holds the dataset in memory is:\n",
    "```Python\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"rating\": 7,\n",
    "        \"polarity\": 1,\n",
    "        \"contents:\": \"Lorem Ipsum\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Later, this should be a `pandas` dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sowell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import utils\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.typing as npt\n",
    "from typing_extensions import Self\n",
    "# from typing import TypeAlias, Tuple\n",
    "TypeAlias = None\n",
    "from typing import Tuple\n",
    "from functools import cached_property\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "POSITIVE_REVIEWS_DIR = \"./data/pos/\"\n",
    "NEGATIVE_REVIEWS_DIR = \"./data/neg/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualDataPoint:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.contents = self._extract_contents()\n",
    "\n",
    "    @cached_property\n",
    "    def basename(self) -> str:\n",
    "        return os.path.basename(self.file_path)\n",
    "\n",
    "    @cached_property\n",
    "    def filename(self) -> str:\n",
    "        return os.path.splitext(self.basename)[0]\n",
    "    \n",
    "    def _extract_contents(self) -> str:\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'contents': self.contents,\n",
    "        }\n",
    "\n",
    "\n",
    "class Review(TextualDataPoint):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "        self.id = self._parse_id()\n",
    "        self.rating = self._parse_rating()\n",
    "        self.polarity = self._determine_polarity()\n",
    "\n",
    "    def _parse_id(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[0])\n",
    "\n",
    "    def _parse_rating(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[1])\n",
    "    \n",
    "    def _determine_polarity(self) -> int:\n",
    "        if self.rating is None:\n",
    "            self.rating = self._parse_rating()\n",
    "\n",
    "        if self.rating <= 4:\n",
    "            return 0\n",
    "        elif self.rating >= 7:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected rating: {self.rating}\")\n",
    "\n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'rating': self.rating,\n",
    "            'polarity': self.polarity,\n",
    "        } | super().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSet:\n",
    "\n",
    "    datapoint_class: TypeAlias = TextualDataPoint\n",
    "\n",
    "    def __init__(self, datapoints: list[datapoint_class]):\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        # To keep track of the current iteration position.\n",
    "        self.index = 0\n",
    "\n",
    "    def first(self) -> datapoint_class:\n",
    "        return self.datapoints[0]\n",
    "\n",
    "    def last(self) -> datapoint_class:\n",
    "        return self.datapoints[-1]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __iter__(self) -> Self:\n",
    "        # Reset the index whenever starting a new iteration.\n",
    "        self.index = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self) -> datapoint_class:\n",
    "        # Make sure there are more datapoints to yield.\n",
    "        if self.index < len(self.datapoints):\n",
    "            result = self.datapoints[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            # No more datapoints -> raise StopIteration exception.\n",
    "            raise StopIteration\n",
    "\n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        return [\n",
    "            datapoint.as_dict()\n",
    "            for datapoint in self.datapoints\n",
    "        ]\n",
    "    \n",
    "    def as_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.as_lower_representation())\n",
    "\n",
    "    def to_csv(self, file_path: str) -> None:\n",
    "        return self.as_df().to_csv(file_path)\n",
    "    \n",
    "\n",
    "class SplitableSet(IterableSet):\n",
    "\n",
    "    def to_csv_as_train_dev_test_sets(\n",
    "            self, \n",
    "            output_dir: str, \n",
    "            target_variable_name: str, \n",
    "            dev_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> None:\n",
    "        train, dev, test = self.as_train_dev_test_dfs(\n",
    "            target_variable_name, dev_test_size, random_state)\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        train.to_csv(output_dir+\"train.csv\")\n",
    "        dev.to_csv(output_dir+\"dev.csv\")\n",
    "        test.to_csv(output_dir+\"test.csv\")\n",
    "\n",
    "    def as_training_dataframe(self, target_variable_name: str) -> pd.DataFrame:\n",
    "        salient_columns = [\"contents\", target_variable_name]\n",
    "        column_rename_map = {\"contents\": \"X\", target_variable_name: \"y\"}\n",
    "\n",
    "        training_dataframe = self.as_df()[salient_columns]\n",
    "        training_dataframe.rename(columns=column_rename_map, inplace=True)\n",
    "\n",
    "        return training_dataframe\n",
    "    \n",
    "    def split_into_train_dev_test_dfs(\n",
    "            self, \n",
    "            target_variable_name: str, \n",
    "            dev_and_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        entire_df = self.as_training_dataframe(target_variable_name)\n",
    "\n",
    "        # Split the data into train and dev+test sets in a ratio of:\n",
    "        #  -> (1-dev_and_test_size):(dev_and_test_size)\n",
    "        initial_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=dev_and_test_size, random_state=random_state)\n",
    "        train_indexes, test_indexes = next(\n",
    "            initial_splitter.split(entire_df.X, entire_df.y))\n",
    "\n",
    "        train_df = entire_df.iloc[train_indexes]\n",
    "        dev_and_test_df = entire_df.iloc[test_indexes]\n",
    "\n",
    "        # Split the dev + test set into dev and test sets in a 50:50 ratio.\n",
    "        final_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.5, random_state=random_state)\n",
    "        dev_indexes, test_indexes = next(\n",
    "            final_splitter.split(dev_and_test_df.X, dev_and_test_df.y))\n",
    "\n",
    "        dev_df = dev_and_test_df.iloc[dev_indexes]\n",
    "        test_df = dev_and_test_df.iloc[test_indexes]\n",
    "\n",
    "        return train_df, dev_df, test_df\n",
    "    \n",
    "    def split_into_train_dev_test_arrays(\n",
    "            self, \n",
    "            target_variable_name: str, \n",
    "            dev_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> Tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n",
    "        train, dev, test = self.split_into_train_dev_test_dfs(\n",
    "            target_variable_name,\n",
    "            dev_test_size,\n",
    "            random_state\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            utils.convert_to_nd_array(train.X),\n",
    "            train.y.values,\n",
    "            utils.convert_to_nd_array(dev.X),\n",
    "            dev.y.values,\n",
    "            utils.convert_to_nd_array(test.X),\n",
    "            test.y.values,\n",
    "        )\n",
    "\n",
    "\n",
    "class DataSet(SplitableSet):\n",
    "\n",
    "    def __init__(self, dirs: list[str]):\n",
    "        super().__init__(None)\n",
    "        self.dirs = dirs\n",
    "        \n",
    "    def load(self) -> Self:\n",
    "        self.datapoints = [\n",
    "            self.datapoint_class(directory + file)\n",
    "            for directory in self.dirs\n",
    "            for file in os.listdir(directory)\n",
    "        ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().as_lower_representation()\n",
    "\n",
    "    def __iter__(self) -> Self:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().__iter__()\n",
    "    \n",
    "\n",
    "class ReviewDataSet(DataSet):\n",
    "\n",
    "    datapoint_class: TypeAlias = Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        # We don't want to modify the original dataset.\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "        # Tokenization is the first preprocessing step of most NLP applications.\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            if isinstance(datapoint.contents, list):\n",
    "                # This datapoint has already been tokenized.\n",
    "                continue\n",
    "\n",
    "            datapoint.contents = nltk.word_tokenize(datapoint.contents)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Generation\n",
    "\n",
    "This section pertains to N-gram Generation and Feature Selection.\n",
    "\n",
    "#### Research\n",
    "[Stemming (NTLK)](https://www.nltk.org/howto/stem.html)  \n",
    "[A comparison of Stemming Algorithms & Lemmatization Algorithms](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
    "- `PorterStemmer` is apparently one of the most aggresive `nltk` stemmers\n",
    "    - It appears the choice of stemmer has a significant impact on performance\n",
    "- `SnowballStemmer` appears to be a lighter middle-ground\n",
    "- Lemmatizers are usually \"lighter\" than stemmers, but they cannot handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet(SplitableSet):\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        super().__init__(dataset.datapoints)\n",
    "\n",
    "    def compare_with(self, other_set: Self):\n",
    "        set1_dp1 = self.first().contents\n",
    "        set2_dp1 = other_set.first().contents\n",
    "        max_length_set1 = len(max(set1_dp1, key=len))\n",
    "\n",
    "        print(\"Comparing feature sets Self and Other:\")\n",
    "        for token1, token2 in zip(set1_dp1, set2_dp1):\n",
    "            empty_space = \" \" * (max_length_set1 - len(token1))\n",
    "            print(f\"Set A: {token1} {empty_space}| Set B: {token2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetGenerator(Preprocessor):\n",
    "\n",
    "    def create_n_grams(self, n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(ngrams(datapoint.contents, n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def create_everygrams(self, max_n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(everygrams(datapoint.contents, max_len=max_n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def to_lowercase(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token.lower() for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self) -> Self:\n",
    "        distinct_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [\n",
    "                # Calling `lower` on `token` because all stopwords are in lowercase.\n",
    "                # This thus removes all stopwords irrespective of their capitalisation.\n",
    "                token for token in datapoint.contents if token.lower() not in distinct_stopwords\n",
    "            ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self) -> Self: \n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in string.punctuation]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lemmatize(self) -> Self:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [lmtzr.lemmatize(token) for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stem(self) -> Self:\n",
    "        # Making the assumption that all datapoints are in English.\n",
    "        stmr = SnowballStemmer(\"english\")\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [stmr.stem(token) for token in datapoint.contents]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetNormalizer:\n",
    "\n",
    "    def __init__(self, feature_set: FeatureSet):\n",
    "        # We don't want to modify the original feature set.\n",
    "        self.feature_set = copy.deepcopy(feature_set)\n",
    "\n",
    "        self.normalized = False\n",
    "        self.shared_vocabulary = self._collect_shared_vocabulary()\n",
    "\n",
    "        self.num_samples = len(self.feature_set)\n",
    "        self.num_features = len(self.shared_vocabulary)\n",
    "\n",
    "    def perform_tf_norm(self, drop_percentile: float = 0) -> FeatureSet:\n",
    "        _, tf_matrix = self._calculate_tf_idf_scores(drop_percentile)\n",
    "\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tf_matrix[doc_idx, :]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def perform_tf_idf_norm(self, drop_percentile: float = 0) -> FeatureSet:\n",
    "        tfidf_matrix, _ = self._calculate_tf_idf_scores(drop_percentile)\n",
    "\n",
    "        # Update datapoint contents with tf-idf values\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tfidf_matrix[doc_idx, :]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def peform_ppmi(self) -> FeatureSet:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _collect_shared_vocabulary(self) -> set:\n",
    "        return {\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in datapoint.contents\n",
    "        }\n",
    "    \n",
    "    def _remove_rare_features(self, tfidf_matrix: npt.NDArray, tf_matrix: npt.NDArray, drop_percentile: float) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "        total_tfidf_per_feature = np.sum(tfidf_matrix, axis=0)\n",
    "        total_tfidf = np.sum(total_tfidf_per_feature)\n",
    "\n",
    "        # Get the indices of total_tfidf_per_feature if it were sorted.\n",
    "        sorted_indices = np.argsort(total_tfidf_per_feature)\n",
    "        # Calculate the cumulative sum along the sorted features.\n",
    "        sorted_cumulative_tfidf = np.cumsum(total_tfidf_per_feature[sorted_indices])\n",
    "\n",
    "        # Determine the cut-off index where the cumulative sum reaches the threshold\n",
    "        # percentage.\n",
    "        threshold_index = np.searchsorted(sorted_cumulative_tfidf, drop_percentile * total_tfidf)\n",
    "\n",
    "        # Use the threshold_index to determine the indices of features to keep.\n",
    "        features_to_keep_indices = sorted_indices[threshold_index:]\n",
    "\n",
    "        # Keep only the columns for features we want to retain.\n",
    "        tfidf_matrix = tfidf_matrix[:, features_to_keep_indices]\n",
    "        tf_matrix = tf_matrix[:, features_to_keep_indices]\n",
    "\n",
    "        return tfidf_matrix, tf_matrix\n",
    "    \n",
    "    def _calculate_tf_idf_scores(self, drop_percentile: float) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "        vocab_to_index = {word: idx for idx, word in enumerate(self.shared_vocabulary)}\n",
    "\n",
    "        # Term frequency matrix. Each row corresponds to a document and each column to a term.\n",
    "        tf_matrix = np.zeros(shape=(self.num_samples, self.num_features), dtype=float)\n",
    "        # This will store a count of how many documents each term appears in, defaulting to 0.\n",
    "        df_counter = defaultdict(int)\n",
    "\n",
    "        # Populate tf_matrix and df_counter\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            # Count term occurences in this document.\n",
    "            term_occurences = Counter(datapoint.contents)\n",
    "            for term, count in term_occurences.items():\n",
    "                if term in vocab_to_index:\n",
    "                    index = vocab_to_index[term]\n",
    "                    # Raw count for TF (to be normalised later)\n",
    "                    tf_matrix[doc_idx, index] = count\n",
    "                    # Increment the df counter.\n",
    "                    df_counter[term] += 1\n",
    "\n",
    "        # Normalise the term frequency matrix row-wise (divide by the number of terms in each document).\n",
    "        doc_lengths = np.array([len(datapoint.contents) for datapoint in self.feature_set])\n",
    "        tf_matrix = tf_matrix / doc_lengths[:, None]\n",
    "\n",
    "        # Transform document frequencies into inverse-document frequencies.\n",
    "        idf_array = np.log(\n",
    "            (self.num_samples) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary]))\n",
    "        )\n",
    "\n",
    "        # Multily the TF matrix by the IDF values to obtain the TF-IDF matrix.\n",
    "        tfidf_matrix = tf_matrix * idf_array\n",
    "\n",
    "        if drop_percentile > 0:\n",
    "            # Remove features that appear most infrequently.\n",
    "            tfidf_matrix, tf_matrix = self._remove_rare_features(tfidf_matrix, tf_matrix, drop_percentile)\n",
    "\n",
    "        return tfidf_matrix, tf_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet([POSITIVE_REVIEWS_DIR, NEGATIVE_REVIEWS_DIR]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_a = FeatureSetGenerator(dataset)\\\n",
    "    .stem()\\\n",
    "    .remove_punctuation()\\\n",
    "    .remove_stopwords()\\\n",
    "    .create_n_grams(1)\n",
    "\n",
    "feature_set_b = FeatureSetGenerator(dataset)\\\n",
    "    .stem()\\\n",
    "    .remove_punctuation()\\\n",
    "    .remove_stopwords()\\\n",
    "    .create_n_grams(1)\n",
    "\n",
    "feature_set_c = FeatureSetGenerator(dataset)\\\n",
    "    .stem()\\\n",
    "    .remove_punctuation()\\\n",
    "    .remove_stopwords()\\\n",
    "    .create_everygrams(2)\n",
    "\n",
    "norm_feature_set_a = FeatureSetNormalizer(feature_set_a).perform_tf_norm()\n",
    "norm_feature_set_b = FeatureSetNormalizer(feature_set_b).perform_tf_idf_norm()\n",
    "norm_feature_set_c = FeatureSetNormalizer(feature_set_c).perform_tf_idf_norm()\n",
    "\n",
    "# Freeing some memory.\n",
    "del feature_set_a\n",
    "del feature_set_b\n",
    "del feature_set_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395824"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_feature_set_c.first().contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = normalized_feature_set.split_into_train_dev_test_arrays(\"polarity\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = normalized_feature_set.split_into_train_dev_test_dfs(\"polarity\", 0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Dataset as CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv_as_train_dev_test_sets(\"./data/bert/\", \"polarity\", 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers import NaiveBayesClassifier\n",
    "from evaluation import FeatureSetComparator\n",
    "\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = []\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    feature_set = FeatureSetGenerator(dataset)\\\n",
    "        .remove_punctuation()\\\n",
    "        .remove_stopwords()\\\n",
    "        .create_n_grams(i)\n",
    "    \n",
    "    norm_feature_set = FeatureSetNormalizer(feature_set).perform_tf_idf_norm(0)\n",
    "    feature_sets.append(norm_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = []\n",
    "y_trains = []\n",
    "X_devs = []\n",
    "y_devs = []\n",
    "\n",
    "for i in range(N):\n",
    "    X_train, y_train, X_dev, y_dev, _, _ = feature_sets[i].split_into_train_dev_test_arrays(\"polarity\", 0.3)\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    y_trains.append(y_train)\n",
    "    X_devs.append(X_dev)\n",
    "    y_devs.append(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator = FeatureSetComparator(X_trains, y_trains)\n",
    "\n",
    "n_gram_performance = comparator.compare(\n",
    "    NaiveBayesClassifier,\n",
    "    X_devs,\n",
    "    y_devs,\n",
    "    {}\n",
    ")\n",
    "\n",
    "n_gram_performance[\"shared_vocab_size\"] = [len(fs.first().contents) for fs in feature_sets]\n",
    "n_gram_performance.index = [f\"N-grams (N = {i+1})\" for i, idx in enumerate(n_gram_performance.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>shared_vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N-grams (N = 1)</th>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.848993</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>50874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-grams (N = 2)</th>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.868644</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>399350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N-grams (N = 3)</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>516967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 accuracy  precision    recall        f1  shared_vocab_size\n",
       "N-grams (N = 1)  0.846667   0.848993  0.843333  0.846154              50874\n",
       "N-grams (N = 2)  0.790000   0.868644  0.683333  0.764925             399350\n",
       "N-grams (N = 3)  0.670000   0.729730  0.540000  0.620690             516967"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
