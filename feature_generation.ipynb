{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- [x] Load all reviews into a data structure in memory\n",
    "- [x] Apply tokenization to each review (base step)\n",
    "- [ ] Choose and apply three pathways through the feature generation process\n",
    "    - _Remember that you will need to justify your choices with reference to the accuracy that is achieved with each feature set_\n",
    "\n",
    "A simple idea for a datastructure that holds the dataset in memory is:\n",
    "```Python\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"rating\": 7,\n",
    "        \"polarity\": 1,\n",
    "        \"contents:\": \"Lorem Ipsum\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "Later, this should be a `pandas` dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\wij21\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.typing as npt\n",
    "from typing_extensions import Self\n",
    "# from typing import TypeAlias, Tuple\n",
    "TypeAlias = None\n",
    "from typing import Tuple\n",
    "from functools import cached_property\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams, everygrams\n",
    "\n",
    "POSITIVE_REVIEWS_DIR = \"./data/pos/\"\n",
    "NEGATIVE_REVIEWS_DIR = \"./data/neg/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualDataPoint:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.contents = self._extract_contents()\n",
    "\n",
    "    @cached_property\n",
    "    def basename(self) -> str:\n",
    "        return os.path.basename(self.file_path)\n",
    "\n",
    "    @cached_property\n",
    "    def filename(self) -> str:\n",
    "        return os.path.splitext(self.basename)[0]\n",
    "    \n",
    "    def _extract_contents(self) -> str:\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "        \n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'contents': self.contents,\n",
    "        }\n",
    "\n",
    "\n",
    "class Review(TextualDataPoint):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "        self.id = self._parse_id()\n",
    "        self.rating = self._parse_rating()\n",
    "        self.polarity = self._determine_polarity()\n",
    "\n",
    "    def _parse_id(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[0])\n",
    "\n",
    "    def _parse_rating(self) -> int:\n",
    "        return int(self.filename.split(\"_\")[1])\n",
    "    \n",
    "    def _determine_polarity(self) -> int:\n",
    "        if self.rating is None:\n",
    "            self.rating = self._parse_rating()\n",
    "\n",
    "        if self.rating <= 4:\n",
    "            return 0\n",
    "        elif self.rating >= 7:\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"unexpected rating: {self.rating}\")\n",
    "\n",
    "    def as_dict(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'rating': self.rating,\n",
    "            'polarity': self.polarity,\n",
    "        } | super().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableSet:\n",
    "\n",
    "    datapoint_class: TypeAlias = TextualDataPoint\n",
    "\n",
    "    def __init__(self, datapoints: list[datapoint_class]):\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        # To keep track of the current iteration position.\n",
    "        self.index = 0\n",
    "\n",
    "    def first(self) -> datapoint_class:\n",
    "        return self.datapoints[0]\n",
    "\n",
    "    def last(self) -> datapoint_class:\n",
    "        return self.datapoints[-1]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __iter__(self) -> Self:\n",
    "        # Reset the index whenever starting a new iteration.\n",
    "        self.index = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self) -> datapoint_class:\n",
    "        # Make sure there are more datapoints to yield.\n",
    "        if self.index < len(self.datapoints):\n",
    "            result = self.datapoints[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            # No more datapoints -> raise StopIteration exception.\n",
    "            raise StopIteration\n",
    "\n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        return [\n",
    "            datapoint.as_dict()\n",
    "            for datapoint in self.datapoints\n",
    "        ]\n",
    "    \n",
    "    def as_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.as_lower_representation())\n",
    "\n",
    "    def to_csv(self, file_path: str) -> None:\n",
    "        return self.as_df().to_csv(file_path)\n",
    "    \n",
    "\n",
    "class SplitableSet(IterableSet):\n",
    "\n",
    "    def to_csv_as_train_dev_test_sets(\n",
    "            self, \n",
    "            output_dir: str, \n",
    "            target_variable_name: str, \n",
    "            dev_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> None:\n",
    "        train, dev, test = self.as_train_dev_test_dfs(\n",
    "            target_variable_name, dev_test_size, random_state)\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        train.to_csv(output_dir+\"train.csv\")\n",
    "        dev.to_csv(output_dir+\"dev.csv\")\n",
    "        test.to_csv(output_dir+\"test.csv\")\n",
    "\n",
    "    def as_training_dataframe(self, target_variable_name: str) -> pd.DataFrame:\n",
    "        salient_columns = [\"contents\", target_variable_name]\n",
    "        column_rename_map = {\"contents\": \"X\", target_variable_name: \"y\"}\n",
    "\n",
    "        training_dataframe = self.as_df()[salient_columns]\n",
    "        training_dataframe.rename(columns=column_rename_map, inplace=True)\n",
    "\n",
    "        return training_dataframe\n",
    "    \n",
    "    def as_train_dev_test_arrays(\n",
    "            self, \n",
    "            target_variable_name: str, \n",
    "            dev_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> Tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n",
    "        train, dev, test = self.as_train_dev_test_dfs(\n",
    "            target_variable_name,\n",
    "            dev_test_size,\n",
    "            random_state\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self._convert_to_nd_array(train.X),\n",
    "            train.y.values,\n",
    "            self._convert_to_nd_array(dev.X),\n",
    "            dev.y.values,\n",
    "            self._convert_to_nd_array(test.X),\n",
    "            test.y.values,\n",
    "        )\n",
    "\n",
    "    def as_train_dev_test_dfs(\n",
    "            self, \n",
    "            target_variable_name: str, \n",
    "            dev_test_size: float = 0.3, \n",
    "            random_state: int = 42\n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        X_y_entire = self.as_training_dataframe(target_variable_name)\n",
    "\n",
    "        # Split the data into train and dev+test sets in a ratio of:\n",
    "        #  -> (1-dev_test_size):(dev_test_size)\n",
    "        initial_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=dev_test_size, random_state=random_state)\n",
    "        train_indexes, test_indexes = next(\n",
    "            initial_splitter.split(X_y_entire.X, X_y_entire.y))\n",
    "\n",
    "        X_y_train = X_y_entire.iloc[train_indexes]\n",
    "        X_y_test_dev = X_y_entire.iloc[test_indexes]\n",
    "\n",
    "        # Split the dev + test set into dev and test sets in a 50:50 ratio.\n",
    "        final_splitter = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.5, random_state=random_state)\n",
    "        dev_indexes, test_indexes = next(\n",
    "            final_splitter.split(X_y_test_dev.X, X_y_test_dev.y))\n",
    "\n",
    "        X_y_dev = X_y_test_dev.iloc[dev_indexes]\n",
    "        X_y_test = X_y_test_dev.iloc[test_indexes]\n",
    "\n",
    "        return X_y_train, X_y_dev, X_y_test\n",
    "\n",
    "    def _convert_to_nd_array(self, series: pd.Series) -> npt.NDArray:\n",
    "        return np.vstack(series.apply(np.array))\n",
    "\n",
    "\n",
    "class DataSet(SplitableSet):\n",
    "\n",
    "    def __init__(self, dirs: list[str]):\n",
    "        super().__init__(None)\n",
    "        self.dirs = dirs\n",
    "        \n",
    "    def load(self) -> Self:\n",
    "        self.datapoints = [\n",
    "            self.datapoint_class(directory + file)\n",
    "            for directory in self.dirs\n",
    "            for file in os.listdir(directory)\n",
    "        ]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def as_lower_representation(self) -> list[dict]:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().as_lower_representation()\n",
    "\n",
    "    def __iter__(self) -> Self:\n",
    "        # Ensure the dataset has been loaded.\n",
    "        if self.datapoints is None:\n",
    "            self.load()\n",
    "\n",
    "        return super().__iter__()\n",
    "    \n",
    "\n",
    "class ReviewDataSet(DataSet):\n",
    "\n",
    "    datapoint_class: TypeAlias = Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        # We don't want to modify the original dataset.\n",
    "        self.dataset = copy.deepcopy(dataset)\n",
    "        # Tokenization is the first preprocessing step of most NLP applications.\n",
    "        self.tokenize()\n",
    "\n",
    "    def tokenize(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            if isinstance(datapoint.contents, list):\n",
    "                # This datapoint has already been tokenized.\n",
    "                continue\n",
    "\n",
    "            datapoint.contents = nltk.word_tokenize(datapoint.contents)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Generation\n",
    "\n",
    "[Stemming (NTLK)](https://www.nltk.org/howto/stem.html)  \n",
    "[A comparison of Stemming Algorithms & Lemmatization Algorithms](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)\n",
    "- `PorterStemmer` is apparently one of the most aggresive `nltk` stemmers\n",
    "    - It appears the choice of stemmer has a significant impact on performance\n",
    "- `SnowballStemmer` appears to be a lighter middle-ground\n",
    "- Lemmatizers are usually \"lighter\" than stemmers, but they cannot handle unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet(SplitableSet):\n",
    "\n",
    "    def __init__(self, dataset: DataSet):\n",
    "        super().__init__(dataset.datapoints)\n",
    "\n",
    "    def compare_with(self, other_set: Self):\n",
    "        set1_dp1 = self.first().contents\n",
    "        set2_dp1 = other_set.first().contents\n",
    "        max_length_set1 = len(max(set1_dp1, key=len))\n",
    "\n",
    "        print(\"Comparing feature sets Self and Other:\")\n",
    "        for token1, token2 in zip(set1_dp1, set2_dp1):\n",
    "            empty_space = \" \" * (max_length_set1 - len(token1))\n",
    "            print(f\"Set A: {token1} {empty_space}| Set B: {token2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetGenerator(Preprocessor):\n",
    "\n",
    "    def create_n_grams(self, n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(ngrams(datapoint.contents, n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def create_everygrams(self, max_n: int) -> FeatureSet:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = list(everygrams(datapoint.contents, max_len=max_n))\n",
    "\n",
    "        return FeatureSet(self.dataset)\n",
    "    \n",
    "    def to_lowercase(self) -> Self:\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token.lower() for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self) -> Self:\n",
    "        distinct_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in distinct_stopwords]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self) -> Self: \n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [token for token in datapoint.contents if token not in string.punctuation]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lemmatize(self) -> Self:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [lmtzr.lemmatize(token) for token in datapoint.contents]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stem(self) -> Self:\n",
    "        # Making the assumption that all datapoints are in English.\n",
    "        stmr = SnowballStemmer(\"english\")\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            datapoint.contents = [stmr.stem(token) for token in datapoint.contents]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Normalisation\n",
    "\n",
    "- This needs to be much faster.\n",
    "- Are we actually calcuating word frequencies or just word occurences (are these different?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSetNormalizer:\n",
    "\n",
    "    def __init__(self, feature_set: FeatureSet):\n",
    "        # We don't want to modify the original feature set.\n",
    "        self.feature_set = copy.deepcopy(feature_set)\n",
    "\n",
    "        self.normalized = False\n",
    "        self.shared_vocabulary = self._collect_shared_vocabulary()\n",
    "\n",
    "        self.num_samples = len(self.feature_set)\n",
    "        self.num_features = len(self.shared_vocabulary)\n",
    "\n",
    "    def perform_tf_norm(self, drop_percentile: float = 0) -> FeatureSet:\n",
    "        _, tf_matrix = self._calculate_tf_idf_scores(drop_percentile)\n",
    "\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tf_matrix[doc_idx, :]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def perform_tf_idf_norm(self, drop_percentile: float = 0) -> FeatureSet:\n",
    "        tfidf_matrix, _ = self._calculate_tf_idf_scores(drop_percentile)\n",
    "\n",
    "        # Update datapoint contents with tf-idf values\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            datapoint.contents = tfidf_matrix[doc_idx, :]\n",
    "\n",
    "        return self.feature_set\n",
    "\n",
    "    def peform_ppmi(self) -> FeatureSet:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _collect_shared_vocabulary(self) -> set:\n",
    "        return {\n",
    "            token\n",
    "            for datapoint in self.feature_set\n",
    "            for token in datapoint.contents\n",
    "        }\n",
    "    \n",
    "    def _remove_rare_features(self, tfidf_matrix: npt.NDArray, tf_matrix: npt.NDArray, drop_percentile: float) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "        total_tfidf_per_feature = np.sum(tfidf_matrix, axis=0)\n",
    "        total_tfidf = np.sum(total_tfidf_per_feature)\n",
    "\n",
    "        # Get the indices of total_tfidf_per_feature if it were sorted.\n",
    "        sorted_indices = np.argsort(total_tfidf_per_feature)\n",
    "        # Calculate the cumulative sum along the sorted features.\n",
    "        sorted_cumulative_tfidf = np.cumsum(total_tfidf_per_feature[sorted_indices])\n",
    "\n",
    "        # Determine the cut-off index where the cumulative sum reaches the threshold\n",
    "        # percentage.\n",
    "        threshold_index = np.searchsorted(sorted_cumulative_tfidf, drop_percentile * total_tfidf)\n",
    "\n",
    "        # Use the threshold_index to determine the indices of features to keep.\n",
    "        features_to_keep_indices = sorted_indices[threshold_index:]\n",
    "\n",
    "        # Keep only the columns for features we want to retain.\n",
    "        tfidf_matrix = tfidf_matrix[:, features_to_keep_indices]\n",
    "        tf_matrix = tf_matrix[:, features_to_keep_indices]\n",
    "\n",
    "        return tfidf_matrix, tf_matrix\n",
    "    \n",
    "    def _calculate_tf_idf_scores(self, drop_percentile: float) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "        vocab_to_index = {word: idx for idx, word in enumerate(self.shared_vocabulary)}\n",
    "\n",
    "        # Term frequency matrix. Each row corresponds to a document and each column to a term.\n",
    "        tf_matrix = np.zeros(shape=(self.num_samples, self.num_features), dtype=float)\n",
    "        # This will store a count of how many documents each term appears in, defaulting to 0.\n",
    "        df_counter = defaultdict(int)\n",
    "\n",
    "        # Populate tf_matrix and df_counter\n",
    "        for doc_idx, datapoint in enumerate(self.feature_set):\n",
    "            # Count term occurences in this document.\n",
    "            term_occurences = Counter(datapoint.contents)\n",
    "            for term, count in term_occurences.items():\n",
    "                if term in vocab_to_index:\n",
    "                    index = vocab_to_index[term]\n",
    "                    # Raw count for TF (to be normalised later)\n",
    "                    tf_matrix[doc_idx, index] = count\n",
    "                    # Increment the df counter.\n",
    "                    df_counter[term] += 1\n",
    "\n",
    "        # Normalise the term frequency matrix row-wise (divide by the number of terms in each document).\n",
    "        doc_lengths = np.array([len(datapoint.contents) for datapoint in self.feature_set])\n",
    "        tf_matrix = tf_matrix / doc_lengths[:, None]\n",
    "\n",
    "        # Transform document frequencies into inverse-document frequencies.\n",
    "        idf_array = np.log(\n",
    "            (self.num_samples) / (1 + np.array([df_counter[term] for term in self.shared_vocabulary]))\n",
    "        )\n",
    "\n",
    "        # Calculate the TF-IDF matrix by multily the TF matrix by the IDF values.\n",
    "        tfidf_matrix = tf_matrix * idf_array\n",
    "\n",
    "        if drop_percentile > 0:\n",
    "            # Remove features that appear most infrequently.\n",
    "            tfidf_matrix, tf_matrix = self._remove_rare_features(tfidf_matrix, tf_matrix, drop_percentile)\n",
    "\n",
    "        return tfidf_matrix, tf_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet([POSITIVE_REVIEWS_DIR, NEGATIVE_REVIEWS_DIR]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_b = FeatureSetGenerator(dataset)\\\n",
    "    .remove_punctuation()\\\n",
    "    .lemmatize()\\\n",
    "    .create_n_grams(1)\n",
    "\n",
    "feature_set_c = FeatureSetGenerator(dataset)\\\n",
    "    .remove_stopwords()\\\n",
    "    .remove_punctuation()\\\n",
    "    .lemmatize()\\\n",
    "    .create_n_grams(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing feature sets Self and Other:\n",
      "Set A: ('Homelessness',) | Set B: ('Homelessness',)\n",
      "Set A: ('or',) | Set B: ('Houselessness',)\n",
      "Set A: ('Houselessness',) | Set B: ('George',)\n",
      "Set A: ('a',) | Set B: ('Carlin',)\n",
      "Set A: ('George',) | Set B: ('stated',)\n",
      "Set A: ('Carlin',) | Set B: ('issue',)\n",
      "Set A: ('stated',) | Set B: ('year',)\n",
      "Set A: ('ha',) | Set B: ('never',)\n",
      "Set A: ('been',) | Set B: ('plan',)\n",
      "Set A: ('an',) | Set B: ('help',)\n",
      "Set A: ('issue',) | Set B: ('street',)\n",
      "Set A: ('for',) | Set B: ('considered',)\n",
      "Set A: ('year',) | Set B: ('human',)\n",
      "Set A: ('but',) | Set B: ('everything',)\n",
      "Set A: ('never',) | Set B: ('going',)\n",
      "Set A: ('a',) | Set B: ('school',)\n",
      "Set A: ('plan',) | Set B: ('work',)\n",
      "Set A: ('to',) | Set B: ('vote',)\n",
      "Set A: ('help',) | Set B: ('matter',)\n",
      "Set A: ('those',) | Set B: ('Most',)\n",
      "Set A: ('on',) | Set B: ('people',)\n",
      "Set A: ('the',) | Set B: ('think',)\n",
      "Set A: ('street',) | Set B: ('homeless',)\n",
      "Set A: ('that',) | Set B: ('lost',)\n",
      "Set A: ('were',) | Set B: ('cause',)\n",
      "Set A: ('once',) | Set B: ('worrying',)\n",
      "Set A: ('considered',) | Set B: ('thing',)\n",
      "Set A: ('human',) | Set B: ('racism',)\n",
      "Set A: ('who',) | Set B: ('war',)\n",
      "Set A: ('did',) | Set B: ('Iraq',)\n",
      "Set A: ('everything',) | Set B: ('pressuring',)\n",
      "Set A: ('from',) | Set B: ('kid',)\n",
      "Set A: ('going',) | Set B: ('succeed',)\n",
      "Set A: ('to',) | Set B: ('technology',)\n",
      "Set A: ('school',) | Set B: ('election',)\n",
      "Set A: ('work',) | Set B: ('inflation',)\n",
      "Set A: ('or',) | Set B: ('worrying',)\n",
      "Set A: ('vote',) | Set B: (\"'ll\",)\n",
      "Set A: ('for',) | Set B: ('next',)\n",
      "Set A: ('the',) | Set B: ('end',)\n",
      "Set A: ('matter',) | Set B: ('streets.',)\n",
      "Set A: ('Most',) | Set B: ('br',)\n",
      "Set A: ('people',) | Set B: ('br',)\n",
      "Set A: ('think',) | Set B: ('But',)\n",
      "Set A: ('of',) | Set B: ('given',)\n",
      "Set A: ('the',) | Set B: ('bet',)\n",
      "Set A: ('homeless',) | Set B: ('live',)\n",
      "Set A: ('a',) | Set B: ('street',)\n",
      "Set A: ('just',) | Set B: ('month',)\n",
      "Set A: ('a',) | Set B: ('without',)\n",
      "Set A: ('lost',) | Set B: ('luxury',)\n",
      "Set A: ('cause',) | Set B: ('home',)\n",
      "Set A: ('while',) | Set B: ('entertainment',)\n",
      "Set A: ('worrying',) | Set B: ('set',)\n",
      "Set A: ('about',) | Set B: ('bathroom',)\n",
      "Set A: ('thing',) | Set B: ('picture',)\n",
      "Set A: ('such',) | Set B: ('wall',)\n",
      "Set A: ('a',) | Set B: ('computer',)\n",
      "Set A: ('racism',) | Set B: ('everything',)\n",
      "Set A: ('the',) | Set B: ('treasure',)\n",
      "Set A: ('war',) | Set B: ('see',)\n",
      "Set A: ('on',) | Set B: (\"'s\",)\n",
      "Set A: ('Iraq',) | Set B: ('like',)\n",
      "Set A: ('pressuring',) | Set B: ('homeless',)\n",
      "Set A: ('kid',) | Set B: ('That',)\n",
      "Set A: ('to',) | Set B: ('Goddard',)\n",
      "Set A: ('succeed',) | Set B: ('Bolt',)\n",
      "Set A: ('technology',) | Set B: (\"'s\",)\n",
      "Set A: ('the',) | Set B: ('lesson.',)\n",
      "Set A: ('election',) | Set B: ('br',)\n",
      "Set A: ('inflation',) | Set B: ('br',)\n",
      "Set A: ('or',) | Set B: ('Mel',)\n",
      "Set A: ('worrying',) | Set B: ('Brooks',)\n",
      "Set A: ('if',) | Set B: ('directs',)\n",
      "Set A: ('they',) | Set B: ('star',)\n",
      "Set A: (\"'ll\",) | Set B: ('Bolt',)\n",
      "Set A: ('be',) | Set B: ('play',)\n",
      "Set A: ('next',) | Set B: ('rich',)\n",
      "Set A: ('to',) | Set B: ('man',)\n",
      "Set A: ('end',) | Set B: ('everything',)\n",
      "Set A: ('up',) | Set B: ('world',)\n",
      "Set A: ('on',) | Set B: ('deciding',)\n",
      "Set A: ('the',) | Set B: ('make',)\n",
      "Set A: ('streets.',) | Set B: ('bet',)\n",
      "Set A: ('br',) | Set B: ('sissy',)\n",
      "Set A: ('br',) | Set B: ('rival',)\n",
      "Set A: ('But',) | Set B: ('Jeffery',)\n",
      "Set A: ('what',) | Set B: ('Tambor',)\n",
      "Set A: ('if',) | Set B: ('see',)\n",
      "Set A: ('you',) | Set B: ('live',)\n",
      "Set A: ('were',) | Set B: ('street',)\n",
      "Set A: ('given',) | Set B: ('thirty',)\n",
      "Set A: ('a',) | Set B: ('day',)\n",
      "Set A: ('bet',) | Set B: ('without',)\n",
      "Set A: ('to',) | Set B: ('luxury',)\n",
      "Set A: ('live',) | Set B: ('Bolt',)\n",
      "Set A: ('on',) | Set B: ('succeeds',)\n",
      "Set A: ('the',) | Set B: ('want',)\n",
      "Set A: ('street',) | Set B: ('future',)\n",
      "Set A: ('for',) | Set B: ('project',)\n",
      "Set A: ('a',) | Set B: ('making',)\n",
      "Set A: ('month',) | Set B: ('building',)\n",
      "Set A: ('without',) | Set B: ('The',)\n",
      "Set A: ('the',) | Set B: ('bet',)\n",
      "Set A: ('luxury',) | Set B: (\"'s\",)\n",
      "Set A: ('you',) | Set B: ('Bolt',)\n",
      "Set A: ('once',) | Set B: ('thrown',)\n",
      "Set A: ('had',) | Set B: ('street',)\n",
      "Set A: ('from',) | Set B: ('bracelet',)\n",
      "Set A: ('a',) | Set B: ('leg',)\n",
      "Set A: ('home',) | Set B: ('monitor',)\n",
      "Set A: ('the',) | Set B: ('every',)\n",
      "Set A: ('entertainment',) | Set B: ('move',)\n",
      "Set A: ('set',) | Set B: ('ca',)\n",
      "Set A: ('a',) | Set B: (\"n't\",)\n",
      "Set A: ('bathroom',) | Set B: ('step',)\n",
      "Set A: ('picture',) | Set B: ('sidewalk',)\n",
      "Set A: ('on',) | Set B: ('He',)\n",
      "Set A: ('the',) | Set B: (\"'s\",)\n",
      "Set A: ('wall',) | Set B: ('given',)\n",
      "Set A: ('a',) | Set B: ('nickname',)\n",
      "Set A: ('computer',) | Set B: ('Pepto',)\n",
      "Set A: ('and',) | Set B: ('vagrant',)\n",
      "Set A: ('everything',) | Set B: (\"'s\",)\n",
      "Set A: ('you',) | Set B: ('written',)\n",
      "Set A: ('once',) | Set B: ('forehead',)\n",
      "Set A: ('treasure',) | Set B: ('Bolt',)\n",
      "Set A: ('to',) | Set B: ('meet',)\n",
      "Set A: ('see',) | Set B: ('character',)\n",
      "Set A: ('what',) | Set B: ('including',)\n",
      "Set A: ('it',) | Set B: ('woman',)\n",
      "Set A: (\"'s\",) | Set B: ('name',)\n",
      "Set A: ('like',) | Set B: ('Molly',)\n",
      "Set A: ('to',) | Set B: ('Lesley',)\n",
      "Set A: ('be',) | Set B: ('Ann',)\n",
      "Set A: ('homeless',) | Set B: ('Warren',)\n",
      "Set A: ('That',) | Set B: ('ex-dancer',)\n",
      "Set A: ('is',) | Set B: ('got',)\n",
      "Set A: ('Goddard',) | Set B: ('divorce',)\n",
      "Set A: ('Bolt',) | Set B: ('losing',)\n",
      "Set A: (\"'s\",) | Set B: ('home',)\n",
      "Set A: ('lesson.',) | Set B: ('pal',)\n",
      "Set A: ('br',) | Set B: ('Sailor',)\n",
      "Set A: ('br',) | Set B: ('Howard',)\n",
      "Set A: ('Mel',) | Set B: ('Morris',)\n",
      "Set A: ('Brooks',) | Set B: ('Fumes',)\n",
      "Set A: ('who',) | Set B: ('Teddy',)\n",
      "Set A: ('directs',) | Set B: ('Wilson',)\n",
      "Set A: ('who',) | Set B: ('already',)\n",
      "Set A: ('star',) | Set B: ('used',)\n",
      "Set A: ('a',) | Set B: ('street',)\n",
      "Set A: ('Bolt',) | Set B: ('They',)\n",
      "Set A: ('play',) | Set B: (\"'re\",)\n",
      "Set A: ('a',) | Set B: ('survivor',)\n",
      "Set A: ('rich',) | Set B: ('Bolt',)\n",
      "Set A: ('man',) | Set B: (\"n't\",)\n",
      "Set A: ('who',) | Set B: ('He',)\n",
      "Set A: ('ha',) | Set B: (\"'s\",)\n",
      "Set A: ('everything',) | Set B: ('used',)\n",
      "Set A: ('in',) | Set B: ('reaching',)\n",
      "Set A: ('the',) | Set B: ('mutual',)\n",
      "Set A: ('world',) | Set B: ('agreement',)\n",
      "Set A: ('until',) | Set B: ('like',)\n",
      "Set A: ('deciding',) | Set B: ('rich',)\n",
      "Set A: ('to',) | Set B: (\"'s\",)\n",
      "Set A: ('make',) | Set B: ('fight',)\n",
      "Set A: ('a',) | Set B: ('flight',)\n",
      "Set A: ('bet',) | Set B: ('kill',)\n",
      "Set A: ('with',) | Set B: ('killed.',)\n",
      "Set A: ('a',) | Set B: ('br',)\n",
      "Set A: ('sissy',) | Set B: ('br',)\n",
      "Set A: ('rival',) | Set B: ('While',)\n",
      "Set A: ('Jeffery',) | Set B: ('love',)\n",
      "Set A: ('Tambor',) | Set B: ('connection',)\n",
      "Set A: ('to',) | Set B: ('Molly',)\n",
      "Set A: ('see',) | Set B: ('Bolt',)\n",
      "Set A: ('if',) | Set B: (\"n't\",)\n",
      "Set A: ('he',) | Set B: ('necessary',)\n",
      "Set A: ('can',) | Set B: ('plot',)\n",
      "Set A: ('live',) | Set B: ('I',)\n",
      "Set A: ('in',) | Set B: ('found',)\n",
      "Set A: ('the',) | Set B: ('``',)\n",
      "Set A: ('street',) | Set B: ('Life',)\n",
      "Set A: ('for',) | Set B: ('Stinks',)\n",
      "Set A: ('thirty',) | Set B: (\"''\",)\n",
      "Set A: ('day',) | Set B: ('one',)\n",
      "Set A: ('without',) | Set B: ('Mel',)\n",
      "Set A: ('the',) | Set B: ('Brooks',)\n",
      "Set A: ('luxury',) | Set B: ('observant',)\n",
      "Set A: ('if',) | Set B: ('film',)\n",
      "Set A: ('Bolt',) | Set B: ('prior',)\n",
      "Set A: ('succeeds',) | Set B: ('comedy',)\n",
      "Set A: ('he',) | Set B: ('show',)\n",
      "Set A: ('can',) | Set B: ('tender',)\n",
      "Set A: ('do',) | Set B: ('side',)\n",
      "Set A: ('what',) | Set B: ('compared',)\n",
      "Set A: ('he',) | Set B: ('slapstick',)\n",
      "Set A: ('want',) | Set B: ('work',)\n",
      "Set A: ('with',) | Set B: ('Blazing',)\n",
      "Set A: ('a',) | Set B: ('Saddles',)\n",
      "Set A: ('future',) | Set B: ('Young',)\n",
      "Set A: ('project',) | Set B: ('Frankenstein',)\n",
      "Set A: ('of',) | Set B: ('Spaceballs',)\n",
      "Set A: ('making',) | Set B: ('matter',)\n",
      "Set A: ('more',) | Set B: ('show',)\n",
      "Set A: ('building',) | Set B: (\"'s\",)\n",
      "Set A: ('The',) | Set B: ('like',)\n",
      "Set A: ('bet',) | Set B: ('something',)\n",
      "Set A: (\"'s\",) | Set B: ('valuable',)\n",
      "Set A: ('on',) | Set B: ('losing',)\n",
      "Set A: ('where',) | Set B: ('next',)\n",
      "Set A: ('Bolt',) | Set B: ('day',)\n",
      "Set A: ('is',) | Set B: ('hand',)\n",
      "Set A: ('thrown',) | Set B: ('making',)\n",
      "Set A: ('on',) | Set B: ('stupid',)\n",
      "Set A: ('the',) | Set B: ('bet',)\n",
      "Set A: ('street',) | Set B: ('like',)\n",
      "Set A: ('with',) | Set B: ('rich',)\n",
      "Set A: ('a',) | Set B: ('people',)\n",
      "Set A: ('bracelet',) | Set B: (\"n't\",)\n",
      "Set A: ('on',) | Set B: ('know',)\n",
      "Set A: ('his',) | Set B: ('money',)\n",
      "Set A: ('leg',) | Set B: ('Maybe',)\n",
      "Set A: ('to',) | Set B: ('give',)\n",
      "Set A: ('monitor',) | Set B: ('homeless',)\n",
      "Set A: ('his',) | Set B: ('instead',)\n",
      "Set A: ('every',) | Set B: ('using',)\n",
      "Set A: ('move',) | Set B: ('like',)\n",
      "Set A: ('where',) | Set B: ('Monopoly',)\n",
      "Set A: ('he',) | Set B: ('money.',)\n",
      "Set A: ('ca',) | Set B: ('br',)\n",
      "Set A: (\"n't\",) | Set B: ('br',)\n",
      "Set A: ('step',) | Set B: ('Or',)\n",
      "Set A: ('off',) | Set B: ('maybe',)\n",
      "Set A: ('the',) | Set B: ('film',)\n",
      "Set A: ('sidewalk',) | Set B: ('inspire',)\n",
      "Set A: ('He',) | Set B: ('help',)\n",
      "Set A: (\"'s\",) | Set B: ('others',)\n"
     ]
    }
   ],
   "source": [
    "feature_set_b.compare_with(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = FeatureSetNormalizer(feature_set_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_feature_set = normalizer.perform_tf_idf_norm(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10492"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_feature_set.first().contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test = normalized_feature_set.as_train_dev_test_arrays(\"polarity\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = normalized_feature_set.as_train_dev_test_dfs(\"polarity\", 0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Dataset as CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv_as_train_dev_test_sets(\"./data/bert/\", \"polarity\", 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
